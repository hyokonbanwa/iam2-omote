{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee6da7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def load_json(file_path):\n",
    "    \"\"\"\n",
    "    Load a JSON file and return its content as a Python dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The content of the JSON file as a dictionary.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e73a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data_ssd/MMAD/MMAD_for_llava-onevision.json\"\n",
    "data = load_json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7328131c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39672\n",
      "dict_keys(['id', 'image', 'conversations', 'Answer', 'Question', 'Options', 'type', 'annotation', 'mask_path', 'similar_templates', 'random_templates'])\n",
      "id Anomaly Detection_DS-MVTec/bottle/image/broken_large/000.png\n",
      "image DS-MVTec/bottle/image/broken_large/000.png\n",
      "conversations [{'from': 'human', 'value': \"Test image:\\n<image>\\nIs there any defect in the object?\\nA. Yes.\\nB. No.\\nAnswer with the option's letter from the given choices directly.\"}, {'from': 'gpt', 'value': 'A'}]\n",
      "Answer A\n",
      "Question Is there any defect in the object?\n",
      "Options {'A': 'Yes.', 'B': 'No.'}\n",
      "type Anomaly Detection\n",
      "annotation True\n",
      "mask_path rbg_mask/broken_large/000_rbg_mask.png\n",
      "similar_templates ['MVTec-AD/bottle/train/good/001.png', 'MVTec-AD/bottle/train/good/061.png', 'MVTec-AD/bottle/train/good/199.png', 'MVTec-AD/bottle/train/good/124.png', 'MVTec-AD/bottle/train/good/149.png', 'MVTec-AD/bottle/train/good/147.png', 'MVTec-AD/bottle/train/good/089.png', 'MVTec-AD/bottle/train/good/066.png']\n",
      "random_templates ['MVTec-AD/bottle/train/good/004.png', 'MVTec-AD/bottle/train/good/032.png', 'MVTec-AD/bottle/train/good/093.png', 'MVTec-AD/bottle/train/good/095.png', 'MVTec-AD/bottle/train/good/104.png', 'MVTec-AD/bottle/train/good/121.png', 'MVTec-AD/bottle/train/good/177.png', 'MVTec-AD/bottle/train/good/191.png']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(data[0].keys())\n",
    "[print(k,v) for k, v in data[0].items()]  # Exclude 'image' key for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6aeb917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39672/39672 [00:00<00:00, 663465.28it/s]\n"
     ]
    }
   ],
   "source": [
    "task_dataset_name_dict = {}\n",
    "for item in tqdm(data):\n",
    "    task= item[\"id\"].split(\"/\")[0].split(\"_\")[0]\n",
    "    dataset_name = item[\"image\"].split(\"/\")[0]\n",
    "    if task not in task_dataset_name_dict:\n",
    "        task_dataset_name_dict[task] = {}\n",
    "    if dataset_name not in task_dataset_name_dict[task]:\n",
    "        task_dataset_name_dict[task][dataset_name] = []\n",
    "    \n",
    "    task_dataset_name_dict[task][dataset_name].append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f849d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_dataset_name_dict keys: dict_keys(['Anomaly Detection', 'Defect Classification', 'Defect Localization', 'Defect Description', 'Defect Analysis', 'Object Classification', 'Object Structure', 'Object Details', 'Object Analysis'])\n",
      "Total number of items across all tasks and datasets: 0\n",
      "Task: Anomaly Detection\n",
      "  Dataset: DS-MVTec, Number of items: 1691\n",
      "  Dataset: MVTec-LOCO, Number of items: 1565\n",
      "  Dataset: VisA, Number of items: 2141\n",
      "  Dataset: GoodsAD, Number of items: 2900\n",
      "\n",
      "Task: Defect Classification\n",
      "  Dataset: DS-MVTec, Number of items: 1205\n",
      "  Dataset: MVTec-LOCO, Number of items: 982\n",
      "  Dataset: VisA, Number of items: 1190\n",
      "  Dataset: GoodsAD, Number of items: 1311\n",
      "\n",
      "Task: Defect Localization\n",
      "  Dataset: DS-MVTec, Number of items: 1193\n",
      "  Dataset: MVTec-LOCO, Number of items: 982\n",
      "  Dataset: VisA, Number of items: 1197\n",
      "  Dataset: GoodsAD, Number of items: 1506\n",
      "\n",
      "Task: Defect Description\n",
      "  Dataset: DS-MVTec, Number of items: 1213\n",
      "  Dataset: MVTec-LOCO, Number of items: 974\n",
      "  Dataset: VisA, Number of items: 1190\n",
      "  Dataset: GoodsAD, Number of items: 1333\n",
      "\n",
      "Task: Defect Analysis\n",
      "  Dataset: DS-MVTec, Number of items: 1205\n",
      "  Dataset: MVTec-LOCO, Number of items: 954\n",
      "  Dataset: VisA, Number of items: 1163\n",
      "  Dataset: GoodsAD, Number of items: 1460\n",
      "\n",
      "Task: Object Classification\n",
      "  Dataset: DS-MVTec, Number of items: 464\n",
      "  Dataset: MVTec-LOCO, Number of items: 544\n",
      "  Dataset: VisA, Number of items: 935\n",
      "  Dataset: GoodsAD, Number of items: 1211\n",
      "\n",
      "Task: Object Structure\n",
      "  Dataset: DS-MVTec, Number of items: 449\n",
      "  Dataset: MVTec-LOCO, Number of items: 527\n",
      "  Dataset: VisA, Number of items: 943\n",
      "  Dataset: GoodsAD, Number of items: 1133\n",
      "\n",
      "Task: Object Details\n",
      "  Dataset: DS-MVTec, Number of items: 458\n",
      "  Dataset: MVTec-LOCO, Number of items: 534\n",
      "  Dataset: VisA, Number of items: 938\n",
      "  Dataset: GoodsAD, Number of items: 1094\n",
      "\n",
      "Task: Object Analysis\n",
      "  Dataset: DS-MVTec, Number of items: 460\n",
      "  Dataset: MVTec-LOCO, Number of items: 562\n",
      "  Dataset: VisA, Number of items: 925\n",
      "  Dataset: GoodsAD, Number of items: 1140\n",
      "\n",
      "Total number of items across all tasks and datasets: 39672\n"
     ]
    }
   ],
   "source": [
    "print(\"task_dataset_name_dict keys:\", task_dataset_name_dict.keys())\n",
    "\n",
    "data_count = 0\n",
    "print(f\"Total number of items across all tasks and datasets: {data_count}\")\n",
    "for task, datasets in task_dataset_name_dict.items():\n",
    "    print(f\"Task: {task}\")\n",
    "    for dataset_name, items in datasets.items():\n",
    "        print(f\"  Dataset: {dataset_name}, Number of items: {len(items)}\")\n",
    "        data_count += len(items)\n",
    "        # Uncomment the next line to see the first item in each dataset\n",
    "        # print(f\"    Example item: {items[0]}\")\n",
    "    print()  # Add a newline for better readability between tasks\n",
    "    \n",
    "print(f\"Total number of items across all tasks and datasets: {data_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bc88d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./MMAD_task_dataset_name_dict.json\"\n",
    "with open(save_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(task_dataset_name_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33094ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./MMAD_task_dataset_name_dict.json\n",
      "Loaded data from ./MMAD_task_dataset_name_dict.json: 9 items\n",
      "task_dataset_name_dict keys: dict_keys(['Anomaly Detection', 'Defect Classification', 'Defect Localization', 'Defect Description', 'Defect Analysis', 'Object Classification', 'Object Structure', 'Object Details', 'Object Analysis'])\n",
      "Total number of items across all tasks and datasets: 0\n",
      "Task: Anomaly Detection\n",
      "  Dataset: DS-MVTec, Number of items: 1691\n",
      "  Dataset: MVTec-LOCO, Number of items: 1565\n",
      "  Dataset: VisA, Number of items: 2141\n",
      "  Dataset: GoodsAD, Number of items: 2900\n",
      "\n",
      "Task: Defect Classification\n",
      "  Dataset: DS-MVTec, Number of items: 1205\n",
      "  Dataset: MVTec-LOCO, Number of items: 982\n",
      "  Dataset: VisA, Number of items: 1190\n",
      "  Dataset: GoodsAD, Number of items: 1311\n",
      "\n",
      "Task: Defect Localization\n",
      "  Dataset: DS-MVTec, Number of items: 1193\n",
      "  Dataset: MVTec-LOCO, Number of items: 982\n",
      "  Dataset: VisA, Number of items: 1197\n",
      "  Dataset: GoodsAD, Number of items: 1506\n",
      "\n",
      "Task: Defect Description\n",
      "  Dataset: DS-MVTec, Number of items: 1213\n",
      "  Dataset: MVTec-LOCO, Number of items: 974\n",
      "  Dataset: VisA, Number of items: 1190\n",
      "  Dataset: GoodsAD, Number of items: 1333\n",
      "\n",
      "Task: Defect Analysis\n",
      "  Dataset: DS-MVTec, Number of items: 1205\n",
      "  Dataset: MVTec-LOCO, Number of items: 954\n",
      "  Dataset: VisA, Number of items: 1163\n",
      "  Dataset: GoodsAD, Number of items: 1460\n",
      "\n",
      "Task: Object Classification\n",
      "  Dataset: DS-MVTec, Number of items: 464\n",
      "  Dataset: MVTec-LOCO, Number of items: 544\n",
      "  Dataset: VisA, Number of items: 935\n",
      "  Dataset: GoodsAD, Number of items: 1211\n",
      "\n",
      "Task: Object Structure\n",
      "  Dataset: DS-MVTec, Number of items: 449\n",
      "  Dataset: MVTec-LOCO, Number of items: 527\n",
      "  Dataset: VisA, Number of items: 943\n",
      "  Dataset: GoodsAD, Number of items: 1133\n",
      "\n",
      "Task: Object Details\n",
      "  Dataset: DS-MVTec, Number of items: 458\n",
      "  Dataset: MVTec-LOCO, Number of items: 534\n",
      "  Dataset: VisA, Number of items: 938\n",
      "  Dataset: GoodsAD, Number of items: 1094\n",
      "\n",
      "Task: Object Analysis\n",
      "  Dataset: DS-MVTec, Number of items: 460\n",
      "  Dataset: MVTec-LOCO, Number of items: 562\n",
      "  Dataset: VisA, Number of items: 925\n",
      "  Dataset: GoodsAD, Number of items: 1140\n",
      "\n",
      "Total number of items across all tasks and datasets: 39672\n"
     ]
    }
   ],
   "source": [
    "save_data = load_json(\"/data_ssd/MMAD/MMAD_task_dataset_name_dict_llava-oenvision.json\")\n",
    "print(f\"Data saved to {save_path}\")\n",
    "print(f\"Loaded data from {save_path}: {len(save_data)} items\")\n",
    "\n",
    "print(\"task_dataset_name_dict keys:\", save_data.keys())\n",
    "\n",
    "data_count = 0\n",
    "print(f\"Total number of items across all tasks and datasets: {data_count}\")\n",
    "for task, datasets in save_data.items():\n",
    "    print(f\"Task: {task}\")\n",
    "    for dataset_name, items in datasets.items():\n",
    "        print(f\"  Dataset: {dataset_name}, Number of items: {len(items)}\")\n",
    "        data_count += len(items)\n",
    "        # Uncomment the next line to see the first item in each dataset\n",
    "        # print(f\"    Example item: {items[0]}\")\n",
    "    print()  # Add a newline for better readability between tasks\n",
    "    \n",
    "print(f\"Total number of items across all tasks and datasets: {data_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0842078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dict_keys_and_items(dictionary):\n",
    "    \"\"\"\n",
    "    Display the keys and items of a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        dictionary (dict): The dictionary to display.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for key, value in dictionary.items():\n",
    "        print(f\"Key: {key}, Value: {value}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db44899e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: sample_id, Value: 5390\n",
      "Key: conversations, Value: [{'from': 'human', 'value': \"<image><image>\\nWhat's the detailed difference between the 2 images? Please list in detail.\"}, {'from': 'gpt', 'value': 'The differences between the two images are:\\n\\n1. In the second image, there are leaves falling from the sunflowers and the surrounding foliage.\\n2. The ground in the second image is covered with a layer of fallen leaves, adding a carpet-like appearance.'}]\n",
      "Key: image, Value: ['HQ-Edit/images/83425.jpg', 'HQ-Edit/images/83426.jpg']\n",
      "Key: choice_list, Value: None\n",
      "Key: metadata, Value: {'dataset': 'HQ-Edit-Diff', 'split': 'train', 'num_sample': 98675, 'task_instruction': \"What's the difference between 2 images?\", 'question_type': 'open-ended'}\n"
     ]
    }
   ],
   "source": [
    "display_dict_keys_and_items(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18bba1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 140/601614 [00:00<14:22, 697.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 601614/601614 [06:06<00:00, 1639.34it/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "image_root_dir = \"/home/omote/local-share-data_ssd/huggingface_dataset/lmms-lab/M4-Instruct-Data\"\n",
    "\n",
    "break_flag = False\n",
    "chcek_data = data[14200:]\n",
    "for d in tqdm(chcek_data):\n",
    "    if type(d[\"image\"]) == str:\n",
    "        image_path_list = [d[\"image\"]]\n",
    "    elif type(d[\"image\"]) == list:\n",
    "        image_path_list = d[\"image\"]\n",
    "        \n",
    "    for image_path in image_path_list:\n",
    "        dataset_name = image_path.split(\"/\")[0]\n",
    "        image_name = image_path[len(dataset_name)+1:]\n",
    "        image_path = os.path.join(image_root_dir,dataset_name,dataset_name,image_name)\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image path does not exist: {image_path}\")\n",
    "            break_flag = True\n",
    "            break\n",
    "    if break_flag:\n",
    "        break\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edffb1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'from': 'human', 'value': \"<image><image>\\nWhat's the detailed difference between the 2 images? Please list in detail.\"}, {'from': 'gpt', 'value': 'The differences between the two images are:\\n\\n1. In the second image, there are leaves falling from the sunflowers and the surrounding foliage.\\n2. The ground in the second image is covered with a layer of fallen leaves, adding a carpet-like appearance.'}]\n"
     ]
    }
   ],
   "source": [
    "print(data[0][\"conversations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2feb7544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_conversation_format(item):\n",
    "    conversation = item[\"conversations\"]\n",
    "    if conversation[0][\"from\"] != \"human\":\n",
    "        return False\n",
    "    \n",
    "    if conversation[-1][\"from\"] != \"gpt\":\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def check_multiple_images(item):\n",
    "    if type(item[\"image\"]) == list and len(item[\"image\"]) > 1:\n",
    "        return True\n",
    "    return False\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09390cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615814/615814 [00:00<00:00, 1060753.37it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "pass_data_list = []\n",
    "not_pass_data_list = []\n",
    "for i, d in enumerate(tqdm(data)):\n",
    "    if not check_conversation_format(d) or not check_multiple_images(d):\n",
    "        not_pass_data_list.append(d)\n",
    "        continue\n",
    "    \n",
    "    pass_data_list.append(d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5679695f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All image paths exist.\n",
      "Total data: 615814\n",
      "Pass data: 610080\n",
      "Not pass data: 5734\n"
     ]
    }
   ],
   "source": [
    "print(\"All image paths exist.\")\n",
    "print(f\"Total data: {len(data)}\")\n",
    "print(f\"Pass data: {len(pass_data_list)}\")\n",
    "print(f\"Not pass data: {len(not_pass_data_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30901b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(not_pass_data_list[0][\"conversations\"][0][\"value\"].count(\"<image\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b9c220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "def check_image_number(item):\n",
    "    conversation = item[\"conversations\"]\n",
    "    image_count = 0\n",
    "\n",
    "    for message in conversation:\n",
    "        image_count += message[\"value\"].count(\"<image>\")\n",
    "\n",
    "    if image_count != len(item[\"image\"]):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "def check_image_in_human_conversation(item):\n",
    "    conversation = item[\"conversations\"]\n",
    "    for message in conversation:            \n",
    "        if \"<image>\" in message[\"value\"] and message[\"from\"] == \"gpt\":\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d43fd4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5734/5734 [00:00<00:00, 1422075.40it/s]\n"
     ]
    }
   ],
   "source": [
    "not_conversation_list = []\n",
    "not_multiple_images_list = []\n",
    "\n",
    "for d in tqdm(not_pass_data_list):\n",
    "    if not check_image_in_human_conversation(d):\n",
    "        not_conversation_list.append(d)\n",
    "        \n",
    "        continue\n",
    "    if not check_image_number(d) and not check_multiple_images(d):\n",
    "        not_multiple_images_list.append(d)\n",
    "        continue\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a925a8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5734\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(not_conversation_list))\n",
    "print(len(not_multiple_images_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f54bb0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'twitter_post', 'id': 0, 'conversations': [{'from': 'gpt', 'value': 'Help me write a Twitter post considering the following images.\\n<image><image><image><image><image><image><image><image><image>'}, {'from': 'human', 'value': '\"Embracing the serenity of island life where the water is as clear as the skies. 🌊☀️ #IslandVibes #CrystalClear #BeachDays #TravelDiaries\"'}], 'image': ['mmchat/images/mw2048_832851e1gy1foxoneylnaj22c02c07wi.jpg', 'mmchat/images/mw2048_832851e1gy1foxop6mkvgj22av2av7wi.jpg', 'mmchat/images/mw2048_832851e1gy1foxoo3z9v0j2276276hdu.jpg', 'mmchat/images/mw2048_832851e1gy1foxonjqlsyj21o02t8x6q.jpg', 'mmchat/images/mw2048_832851e1gy1foxoswo4w9j229d29db2h.jpg', 'mmchat/images/mw2048_832851e1gy1foxongl7khj212p0t0wog.jpg', 'mmchat/images/mw2048_832851e1gy1foxou97bjvj229d29d7wk.jpg', 'mmchat/images/mw2048_832851e1gy1foxonujr34j22c02c1kjn.jpg', 'mmchat/images/mw2048_832851e1gy1foxonbhlqoj22z228a7wi.jpg'], 'metadata': {'dataset': 'twitter_post', 'split': 'train', 'num_sample': 0, 'task_instruction': '', 'question_type': 'open-ended'}}\n"
     ]
    }
   ],
   "source": [
    "print(not_conversation_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c0d51072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_gpt_and_human_conversation(item):\n",
    "    \"\"\"\n",
    "    Swap the 'from' field of the first and last conversation messages.\n",
    "\n",
    "    Parameters:\n",
    "        item (dict): The item containing conversations.\n",
    "\n",
    "    Returns:\n",
    "        dict: The item with swapped conversation roles.\n",
    "    \"\"\"\n",
    "    conversation = item[\"conversations\"]\n",
    "    for i,message in enumerate(conversation):\n",
    "        if message[\"from\"] == \"human\" and i % 2 == 1:\n",
    "            conversation[i][\"from\"] = \"gpt\"\n",
    "        elif message[\"from\"] == \"gpt\" and i % 2 == 0:\n",
    "            conversation[i][\"from\"] = \"human\"\n",
    "        else:\n",
    "            print(item)\n",
    "            raise ValueError(f\"Unknown conversation role: {message['from']}\")\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0583d268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5734 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5734/5734 [00:00<00:00, 27124.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "swap_data_list = []\n",
    "for d in tqdm(not_conversation_list):\n",
    "    swapped_item = swap_gpt_and_human_conversation(deepcopy(d))\n",
    "    swap_data_list.append(swapped_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e0fb92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'twitter_post', 'id': 0, 'conversations': [{'from': 'human', 'value': 'Help me write a Twitter post considering the following images.\\n<image><image><image><image><image><image><image><image><image>'}, {'from': 'gpt', 'value': '\"Embracing the serenity of island life where the water is as clear as the skies. 🌊☀️ #IslandVibes #CrystalClear #BeachDays #TravelDiaries\"'}], 'image': ['mmchat/images/mw2048_832851e1gy1foxoneylnaj22c02c07wi.jpg', 'mmchat/images/mw2048_832851e1gy1foxop6mkvgj22av2av7wi.jpg', 'mmchat/images/mw2048_832851e1gy1foxoo3z9v0j2276276hdu.jpg', 'mmchat/images/mw2048_832851e1gy1foxonjqlsyj21o02t8x6q.jpg', 'mmchat/images/mw2048_832851e1gy1foxoswo4w9j229d29db2h.jpg', 'mmchat/images/mw2048_832851e1gy1foxongl7khj212p0t0wog.jpg', 'mmchat/images/mw2048_832851e1gy1foxou97bjvj229d29d7wk.jpg', 'mmchat/images/mw2048_832851e1gy1foxonujr34j22c02c1kjn.jpg', 'mmchat/images/mw2048_832851e1gy1foxonbhlqoj22z228a7wi.jpg'], 'metadata': {'dataset': 'twitter_post', 'split': 'train', 'num_sample': 0, 'task_instruction': '', 'question_type': 'open-ended'}}\n",
      "{'datasource': 'twitter_post', 'id': 0, 'conversations': [{'from': 'gpt', 'value': 'Help me write a Twitter post considering the following images.\\n<image><image><image><image><image><image><image><image><image>'}, {'from': 'human', 'value': '\"Embracing the serenity of island life where the water is as clear as the skies. 🌊☀️ #IslandVibes #CrystalClear #BeachDays #TravelDiaries\"'}], 'image': ['mmchat/images/mw2048_832851e1gy1foxoneylnaj22c02c07wi.jpg', 'mmchat/images/mw2048_832851e1gy1foxop6mkvgj22av2av7wi.jpg', 'mmchat/images/mw2048_832851e1gy1foxoo3z9v0j2276276hdu.jpg', 'mmchat/images/mw2048_832851e1gy1foxonjqlsyj21o02t8x6q.jpg', 'mmchat/images/mw2048_832851e1gy1foxoswo4w9j229d29db2h.jpg', 'mmchat/images/mw2048_832851e1gy1foxongl7khj212p0t0wog.jpg', 'mmchat/images/mw2048_832851e1gy1foxou97bjvj229d29d7wk.jpg', 'mmchat/images/mw2048_832851e1gy1foxonujr34j22c02c1kjn.jpg', 'mmchat/images/mw2048_832851e1gy1foxonbhlqoj22z228a7wi.jpg'], 'metadata': {'dataset': 'twitter_post', 'split': 'train', 'num_sample': 0, 'task_instruction': '', 'question_type': 'open-ended'}}\n"
     ]
    }
   ],
   "source": [
    "print(swap_data_list[0])\n",
    "print(not_conversation_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5129789",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pass_data_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m path = \u001b[33m\"\u001b[39m\u001b[33m/data_ssd/M4-Instruct-Data/m4_instruct_annotations_fixed.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m saved_data = \u001b[43mpass_data_list\u001b[49m.extend(swap_data_list)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal saved data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pass_data_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'pass_data_list' is not defined"
     ]
    }
   ],
   "source": [
    "path = \"/data_ssd/M4-Instruct-Data/m4_instruct_annotations_fixed.json\"\n",
    "\n",
    "saved_data = pass_data_list.extend(swap_data_list)\n",
    "print(f\"Total saved data: {len(pass_data_list)}\")\n",
    "\n",
    "import os\n",
    "if not os.path.exists(os.path.dirname(path)):\n",
    "    os.makedirs(os.path.dirname(path))\n",
    "with open(path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(pass_data_list, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f86161d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed data length: 621548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 621548/621548 [07:41<00:00, 1345.43it/s] \n",
      "100%|██████████| 621548/621548 [00:00<00:00, 673190.70it/s]\n"
     ]
    }
   ],
   "source": [
    "fixed_data = load_json(path)\n",
    "print(f\"Fixed data length: {len(fixed_data)}\")\n",
    "\n",
    "image_root_dir = \"/data_ssd/llava-onevision-data-symbolic-link\"\n",
    "\n",
    "break_flag = False\n",
    "for d in tqdm(fixed_data):\n",
    "    if type(d[\"image\"]) == str:\n",
    "        image_path_list = [d[\"image\"]]\n",
    "    elif type(d[\"image\"]) == list:\n",
    "        image_path_list = d[\"image\"]\n",
    "        \n",
    "    for image_path in image_path_list:\n",
    "        image_path = os.path.join(image_root_dir, image_path)\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image path does not exist: {image_path}\")\n",
    "            break_flag = True\n",
    "            break\n",
    "    if break_flag:\n",
    "        break\n",
    " \n",
    "from tqdm import tqdm\n",
    "pass_data_list = []\n",
    "not_pass_data_list = []\n",
    "for i, d in enumerate(tqdm(fixed_data)):\n",
    "    if not check_conversation_format(d) or not check_multiple_images(d):\n",
    "        not_pass_data_list.append(d)\n",
    "        continue\n",
    "    \n",
    "    pass_data_list.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfb21163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(not_pass_data_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5f71b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

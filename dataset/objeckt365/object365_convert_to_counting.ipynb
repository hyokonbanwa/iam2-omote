{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d0b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import regex as re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_json(file_path):\n",
    "    \"\"\"\n",
    "    Load a JSON file and return its content as a Python dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The content of the JSON file as a dictionary.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "def save_json(data, file_path):\n",
    "    \"\"\"\n",
    "    Save a Python dictionary to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): The data to save.\n",
    "        file_path (str): The path where the JSON file will be saved.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b8212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"/data_ssd/object365/not-converted-counting_object365_for_llava-onevision.json\"\n",
    "json_data = load_json(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6b9ba03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: f859081e-ef03-49c7-a1a1-a59d15d69300\n",
      "image: objects365/train/patch3/objects365_v1_00144532.jpg\n",
      "conversations: [{'from': 'human', 'value': '<image>\\nPlease output bbox coordinates and names of every item in this image.'}, {'from': 'gpt', 'value': 'Person: [0.583,0.309,0.682,0.448]\\nPerson: [0.617,0.346,0.797,0.640]\\nPerson: [0.380,0.295,0.696,0.999]\\nPerson: [-0.000,0.298,0.224,0.999]\\nPerson: [0.301,0.322,0.372,0.537]\\nPerson: [0.330,0.332,0.405,0.626]\\nPerson: [0.360,0.329,0.472,0.692]\\nBook: [0.623,0.525,0.699,0.567]\\nBook: [0.204,0.488,0.275,0.511]\\nBook: [0.185,0.508,0.261,0.534]\\nBook: [0.660,0.399,0.692,0.427]\\nBelt: [0.392,0.806,0.496,0.872]\\nBelt: [0.816,0.583,0.839,0.603]\\nBelt: [0.869,0.663,0.936,0.676]\\nPen/Pencil: [0.280,0.512,0.312,0.560]\\nPen/Pencil: [0.235,0.434,0.258,0.469]\\nPerson: [0.727,0.340,0.778,0.478]\\nBook: [0.711,0.471,0.814,0.495]\\nPerson: [0.038,0.299,0.316,0.946]\\nPerson: [0.765,0.288,0.998,0.860]'}]\n"
     ]
    }
   ],
   "source": [
    "item = json_data[0]\n",
    "for k,v in item.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da51d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def make_conversation(id,image_path,question,answer,image_folder_root=None):\n",
    "    if image_folder_root is not None:\n",
    "        image_path = os.path.join(image_folder_root, image_path)\n",
    "    return_data =   {\n",
    "        \"id\": id,\n",
    "        \"image\": image_path,\n",
    "        \"conversations\": [\n",
    "        {\n",
    "            \"from\": \"human\",\n",
    "            \"value\": f\"<image>\\n{question}\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"gpt\",\n",
    "            \"value\": answer\n",
    "        },\n",
    "        ]\n",
    "    }\n",
    "    return return_data\n",
    "\n",
    "def make_answer(detection_answer):\n",
    "    object_list = re.split(r\": \\[.*\\]\", detection_answer)\n",
    "    object_list = [o.strip(\"\\n\") for o in object_list if len(o.strip(\"\\n\")) > 0]\n",
    "    object_counter = Counter(object_list).most_common()\n",
    "\n",
    "    text = \"\"\n",
    "    for obj, count in object_counter:\n",
    "        text += f\"{obj}: {count}\\n\"\n",
    "\n",
    "    return text.strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa39d7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 871145/871145 [00:30<00:00, 28316.17it/s]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(json_data):\n",
    "    item[\"conversations\"][0][\"value\"] = \"<image>\\nPlease output how many objects are present in the image for each object type.\"\n",
    "    item[\"conversations\"][1][\"value\"] = make_answer(item[\"conversations\"][1][\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25b1623e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items: 871145\n"
     ]
    }
   ],
   "source": [
    "saved_data = load_json(json_path)\n",
    "print(f\"Total items: {len(saved_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbd0bf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'f859081e-ef03-49c7-a1a1-a59d15d69300', 'image': 'objects365/train/patch3/objects365_v1_00144532.jpg', 'conversations': [{'from': 'human', 'value': '<image>\\nPlease output bbox coordinates and names of every item in this image.'}, {'from': 'gpt', 'value': 'Person: [0.583,0.309,0.682,0.448]\\nPerson: [0.617,0.346,0.797,0.640]\\nPerson: [0.380,0.295,0.696,0.999]\\nPerson: [-0.000,0.298,0.224,0.999]\\nPerson: [0.301,0.322,0.372,0.537]\\nPerson: [0.330,0.332,0.405,0.626]\\nPerson: [0.360,0.329,0.472,0.692]\\nBook: [0.623,0.525,0.699,0.567]\\nBook: [0.204,0.488,0.275,0.511]\\nBook: [0.185,0.508,0.261,0.534]\\nBook: [0.660,0.399,0.692,0.427]\\nBelt: [0.392,0.806,0.496,0.872]\\nBelt: [0.816,0.583,0.839,0.603]\\nBelt: [0.869,0.663,0.936,0.676]\\nPen/Pencil: [0.280,0.512,0.312,0.560]\\nPen/Pencil: [0.235,0.434,0.258,0.469]\\nPerson: [0.727,0.340,0.778,0.478]\\nBook: [0.711,0.471,0.814,0.495]\\nPerson: [0.038,0.299,0.316,0.946]\\nPerson: [0.765,0.288,0.998,0.860]'}]}\n"
     ]
    }
   ],
   "source": [
    "print(saved_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd1eb752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'f859081e-ef03-49c7-a1a1-a59d15d69300', 'image': 'objects365/train/patch3/objects365_v1_00144532.jpg', 'conversations': [{'from': 'human', 'value': '<image>\\nPlease output how many objects are present in the image for each object type.'}, {'from': 'gpt', 'value': 'Person: 10\\nBook: 5\\nBelt: 3\\nPen/Pencil: 2'}]}\n"
     ]
    }
   ],
   "source": [
    "print(json_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f75bfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted data saved to /data_ssd/object365/counting_converted_object365_for_llava-onevision.json\n"
     ]
    }
   ],
   "source": [
    "save_json_path = \"/data_ssd/object365/counting_object365_for_llava-onevision.json\"\n",
    "save_json(json_data, save_json_path)\n",
    "print(f\"Converted data saved to {save_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae223e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffdfda2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b9eb8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person: [0.583,0.309,0.682,0.448]\n",
      "Person: [0.617,0.346,0.797,0.640]\n",
      "Person: [0.380,0.295,0.696,0.999]\n",
      "Person: [-0.000,0.298,0.224,0.999]\n",
      "Person: [0.301,0.322,0.372,0.537]\n",
      "Person: [0.330,0.332,0.405,0.626]\n",
      "Person: [0.360,0.329,0.472,0.692]\n",
      "Book: [0.623,0.525,0.699,0.567]\n",
      "Book: [0.204,0.488,0.275,0.511]\n",
      "Book: [0.185,0.508,0.261,0.534]\n",
      "Book: [0.660,0.399,0.692,0.427]\n",
      "Belt: [0.392,0.806,0.496,0.872]\n",
      "Belt: [0.816,0.583,0.839,0.603]\n",
      "Belt: [0.869,0.663,0.936,0.676]\n",
      "Pen/Pencil: [0.280,0.512,0.312,0.560]\n",
      "Pen/Pencil: [0.235,0.434,0.258,0.469]\n",
      "Person: [0.727,0.340,0.778,0.478]\n",
      "Book: [0.711,0.471,0.814,0.495]\n",
      "Person: [0.038,0.299,0.316,0.946]\n",
      "Person: [0.765,0.288,0.998,0.860]\n"
     ]
    }
   ],
   "source": [
    "answer = item[\"conversations\"][1][\"value\"]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6959dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Person', '0.583,0.309,0.682,0.448]\\nPerson', '0.617,0.346,0.797,0.640]\\nPerson', '0.380,0.295,0.696,0.999]\\nPerson', '-0.000,0.298,0.224,0.999]\\nPerson', '0.301,0.322,0.372,0.537]\\nPerson', '0.330,0.332,0.405,0.626]\\nPerson', '0.360,0.329,0.472,0.692]\\nBook', '0.623,0.525,0.699,0.567]\\nBook', '0.204,0.488,0.275,0.511]\\nBook', '0.185,0.508,0.261,0.534]\\nBook', '0.660,0.399,0.692,0.427]\\nBelt', '0.392,0.806,0.496,0.872]\\nBelt', '0.816,0.583,0.839,0.603]\\nBelt', '0.869,0.663,0.936,0.676]\\nPen/Pencil', '0.280,0.512,0.312,0.560]\\nPen/Pencil', '0.235,0.434,0.258,0.469]\\nPerson', '0.727,0.340,0.778,0.478]\\nBook', '0.711,0.471,0.814,0.495]\\nPerson', '0.038,0.299,0.316,0.946]\\nPerson', '0.765,0.288,0.998,0.860]']\n",
      "['Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Person', 'Book', 'Book', 'Book', 'Book', 'Belt', 'Belt', 'Belt', 'Pen/Pencil', 'Pen/Pencil', 'Person', 'Book', 'Person', 'Person']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "print(answer.split(\": [\"))\n",
    "object_list = re.split(r\": \\[.*\\]\", answer)\n",
    "object_list = [o.strip(\"\\n\") for o in object_list if len(o.strip(\"\\n\")) > 0]\n",
    "print(object_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d2e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Person', 10), ('Book', 5), ('Belt', 3), ('Pen/Pencil', 2)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "object_counter = Counter(object_list).most_common()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978da346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b558d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omote/cluster_project/iam2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc4102b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_dir = \"/data_ssd/huggingface_dataset\"\n",
    "cache_dir = \"/data_ssd/huggingface_cache\"\n",
    "\n",
    "dataset_id = os.path.join(dataset_root_dir,\"lmms-lab/LLaVA-OneVision-Data\")\n",
    "\n",
    "# dataset = load_dataset(dataset_id, cache_dir=cache_dir,name=\"CLEVR-Math(MathV360K)\",split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4952597",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset\u001b[49m[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85398167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n"
     ]
    }
   ],
   "source": [
    "dataset_name = ['CLEVR-Math(MathV360K)', 'Evol-Instruct-GPT4-Turbo', 'FigureQA(MathV360K)', 'GEOS(MathV360K)', 'GeoQA+(MathV360K)', 'Geometry3K(MathV360K)', 'IconQA(MathV360K)', 'MapQA(MathV360K)', 'MathV360K_TQA', 'MathV360K_VQA-AS', 'MathV360K_VQA-RAD', 'PMC-VQA(MathV360K)', 'Super-CLEVR(MathV360K)', 'TabMWP(MathV360K)', 'UniGeo(MathV360K)', 'VisualWebInstruct(filtered)', 'VizWiz(MathV360K)', 'ai2d(cauldron,llava_format)', 'ai2d(gpt4v)', 'ai2d(internvl)', 'allava_instruct_laion4v', 'allava_instruct_vflan4v', 'aokvqa(cauldron,llava_format)', 'chart2text(cauldron)', 'chartqa(cauldron,llava_format)', 'chrome_writting', 'clevr(cauldron,llava_format)', 'diagram_image_to_text(cauldron)', 'dvqa(cauldron,llava_format)', 'figureqa(cauldron,llava_format)', 'geo170k(align)', 'geo170k(qa)', 'geo3k', 'geomverse(cauldron)', 'hateful_memes(cauldron,llava_format)', 'hitab(cauldron,llava_format)', 'hme100k', 'iam(cauldron)', 'iconqa(cauldron,llava_format)', 'iiit5k', 'image_textualization(filtered)', 'infographic(gpt4v)', 'infographic_vqa', 'infographic_vqa_llava_format', 'intergps(cauldron,llava_format)', 'k12_printing', 'llava_wild_4v_12k_filtered', 'llava_wild_4v_39k_filtered', 'llavar_gpt4_20k', 'lrv_chart', 'lrv_normal(filtered)', 'magpie_pro(l3_80b_mt)', 'magpie_pro(l3_80b_st)', 'magpie_pro(qwen2_72b_st)', 'mapqa(cauldron,llava_format)', 'mathqa', 'mavis_math_metagen', 'mavis_math_rule_geo', 'multihiertt(cauldron)', 'orand_car_a', 'raven(cauldron)', 'rendered_text(cauldron)', 'robut_sqa(cauldron)', 'robut_wikisql(cauldron)', 'robut_wtq(cauldron,llava_format)', 'scienceqa(cauldron,llava_format)', 'scienceqa(nona_context)', 'screen2words(cauldron)', 'sharegpt4o', 'sharegpt4v(coco)', 'sharegpt4v(knowledge)', 'sharegpt4v(llava)', 'sharegpt4v(sam)', 'sroie', 'st_vqa(cauldron,llava_format)', 'tabmwp(cauldron)', 'tallyqa(cauldron,llava_format)', 'textcaps', 'textocr(gpt4v)', 'tqa(cauldron,llava_format)', 'ureader_cap', 'ureader_ie', 'vision_flan(filtered)', 'vistext(cauldron)', 'visual7w(cauldron,llava_format)', 'visualmrc(cauldron)', 'vqarad(cauldron,llava_format)', 'vsr(cauldron,llava_format)', 'websight(cauldron)']\n",
    "print(len(dataset_name))\n",
    "not_found = ['llava_wild_4v_39k_filtered', 'MathV360K_VQA-RAD', 'MathV360K_VQA-AS', 'Evol-Instruct-GPT4-Turbo', 'llava_wild_4v_12k_filtered', 'MathV360K_TQA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e473c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 57284/57284 [00:29<00:00, 1929.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#dataset = load_dataset(dataset_id, cache_dir=cache_dir, name=\"image_textualization(filtered)\", split=\"train\")\n",
    "dataset = load_dataset(dataset_id, cache_dir=cache_dir, name=\"sharegpt4o\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "390666d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 48647/57284 [02:26<00:21, 396.00it/s]/home/omote/cluster_project/iam2/.venv/lib/python3.12/site-packages/PIL/Image.py:3442: DecompressionBombWarning: Image size (138240000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "100%|██████████| 57284/57284 [02:44<00:00, 348.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "prompt_list = []\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    prompt = dataset[i][\"conversations\"][0][\"value\"]\n",
    "    if prompt not in prompt_list:\n",
    "        prompt_list.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "450bd978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "['<image>\\nPlease describe in depth what this image is about.', '<image>\\nCan you explain in detail what is happening in the picture?', '<image>\\nPlease explain in detail the scene depicted in the picture.', '<image>\\nPlease describe in detail the dynamic and static elements in this image.', '<image>\\nPlease describe what is happening in the image.', '<image>\\nCan you describe all the objects and characters in the picture?', '<image>\\nDescribe this image.', '<image>\\nPlease conduct an in-depth analysis of the scene in the picture.', '<image>\\nPlease elaborate on what this image shows.', '<image>\\nDescribe everything in the image', '<image>\\nProvide a detailed description of the presented image.', '<image>\\nWhat scene is mainly depicted in the picture?', '<image>\\nDescribe the image, paying attention to its inner details.', '<image>\\nWhat items or people are prominent in the picture?', '<image>\\nDescribe the content of a given image in detail', '<image>\\nPlease describe specifically what you observed in the picture and the possible scenes they might form.', '<image>\\nWhat objects and people are shown in the picture?', '<image>\\nPlease use detailed words to describe what the picture is about.', '<image>\\nPlease describe specifically what this image is about.', '<image>\\nPlease explain the scene depicted in the picture.', '<image>\\nWhat scene is this picture depicting?', '<image>\\nDescribe every detail in the picture.', '<image>\\nWhat objects and people are in the picture?', '<image>\\nWhat are the striking details of this image?', '<image>\\nWhat is compelling about this image?', '<image>\\nProvide a detailed description of the image.', '<image>\\nProvide a detailed description of the main elements in the image.', '<image>\\nPlease describe the scene depicted in this picture.', '<image>\\nCan you describe what this image is about?', '<image>\\nDescribe the given image', '<image>\\nWhat elements and people are included in the image?', '<image>\\nPlease explain the visual content of the image in detail.', '<image>\\nPlease describe the content of the picture.', '<image>\\nCan you explain what this image shows?', '<image>\\nCan you analyze and elaborate on all the elements and details shown in this image?', '<image>\\nWrite a detailed and comprehensive description of the image.', '<image>\\nCan you list all the objects and people in the picture?', '<image>\\nDescribe what the picture shows?', '<image>\\nDescribe all the elements in the picture.', '<image>\\nCan you describe the main elements in the image?', '<image>\\nPlease interpret and describe each detail of this image and the overall scene they create.', '<image>\\nPlease describe the situation in the picture as clearly as possible.', '<image>\\nPlease describe the event or scene this image shows.', '<image>\\nAnalyze and describe in detail the visual elements in this image.', '<image>\\nCan you describe this photo in detail?', '<image>\\nPlease describe the scene in the picture.', '<image>\\nWhat details are worth noting in the picture?']\n"
     ]
    }
   ],
   "source": [
    "print(len(prompt_list))\n",
    "print(prompt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e1e5629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: llava_wild_4v_39k_filtered\n",
      "Loading dataset: MathV360K_VQA-RAD\n",
      "Loading dataset: MathV360K_VQA-AS\n",
      "Loading dataset: Evol-Instruct-GPT4-Turbo\n",
      "Loading dataset: llava_wild_4v_12k_filtered\n",
      "Loading dataset: MathV360K_TQA\n"
     ]
    }
   ],
   "source": [
    "for name in not_found:\n",
    "    print(f\"Loading dataset: {name}\")\n",
    "    dataset = load_dataset(dataset_id, cache_dir=cache_dir, name=name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d2ff673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: Dataset({\n",
      "    features: ['id', 'image', 'conversations', 'data_source'],\n",
      "    num_rows: 25104\n",
      "})\n",
      "id: 1b96a7e70cf75b79\n",
      "image: <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x841 at 0x14E822FD5CD0>\n",
      "conversations: [{'from': 'human', 'value': '<image>\\nDescribe this image in detail.'}, {'from': 'gpt', 'value': 'The image showcases a breathtaking landscape with a towering mountain under a vibrant blue sky dotted with clouds, a tranquil body of water, and patches of colorful land. Overlaying the scenic view is a biblical quote from Psalm 119:96 in the N.I.V. version, reading \"To all perfection I see a limit but your commands are boundless.\" In the lower left corner, the photographer\\'s credit, \"Tito Balangue Photography - 2011,\" is displayed, while the lower right corner indicates the photo\\'s subject as \"Landscape Photo - Lao P.D.R.\"'}]\n",
      "data_source: textocr(gpt4v)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "print(\"train_dataset:\", train_dataset)\n",
    "\n",
    "train_data = train_dataset[0]\n",
    "\n",
    "for key, value in train_data.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2598eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(dataset_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9af97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_json(file_path):\n",
    "    \"\"\"\n",
    "    Load a JSON file and return its content as a Python dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The content of the JSON file as a dictionary.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "def save_json(data, file_path):\n",
    "    \"\"\"\n",
    "    Save a Python dictionary to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): The data to save.\n",
    "        file_path (str): The path where the JSON file will be saved.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10727c3e",
   "metadata": {},
   "source": [
    "# change_json_image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a72847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4864\n",
      "dict_keys(['id', 'conversations', 'data_source', 'image'])\n",
      "LLaVA-OneVision-Data/ai2d(gpt4v)/train/ai2d_azuregpt_4145.png\n"
     ]
    }
   ],
   "source": [
    "path = \"/data_ssd/USER_DATA/omote/iam2/LLaVA-OneVision-Data/ai2d(gpt4v)/ai2d(gpt4v).json\"\n",
    "path = \"/data_ssd/USER_DATA/omote/iam2/LLaVA-OneVision-Data/ai2d(gpt4v)/ai2d(gpt4v)_relative_path.json\"\n",
    "data = load_json(path)\n",
    "print(len(data))\n",
    "print(data[0].keys())\n",
    "print(data[0][\"image\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbada9d",
   "metadata": {},
   "source": [
    "# ureader_kg_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d05aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252954\n",
      "dict_keys(['id', 'conversations', 'data_source', 'image'])\n",
      "ureader-instruction-1.0/ChartQA/train/png/00006834003065.png\n"
     ]
    }
   ],
   "source": [
    "path = \"/data_ssd/USER_DATA/omote/iam2/LLaVA-OneVision-Data/ureader_qa/ureader_qa_processed.json\"\n",
    "data = load_json(path)\n",
    "print(len(data))\n",
    "print(data[0].keys())\n",
    "print(data[0][\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83435900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/252954 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252954/252954 [00:15<00:00, 16628.54it/s]\n"
     ]
    }
   ],
   "source": [
    "image_folder = \"/data_ssd/USER_DATA/omote/iam2/LLaVA-OneVision-Data/ureader_qa\"\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "for item in tqdm(data):\n",
    "    image_path = item[\"image\"]\n",
    "    if not os.path.exists(os.path.join(image_folder, image_path)):\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8190cb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252954\n",
      "{'id': '00006834003065', 'conversations': [{'from': 'human', 'value': '<image>\\nWhich country has longest bar?\\nAnswer the question using a single word or phrase.'}, {'from': 'gpt', 'value': 'Nigeria'}], 'data_source': 'ureader_qa', 'image': 'LLaVA-OneVision-Data/ureader_qa/ureader-instruction-1.0/ChartQA/train/png/00006834003065.png'}\n"
     ]
    }
   ],
   "source": [
    "new_json_data = []\n",
    "\n",
    "for item in data:\n",
    "    item[\"image\"] = os.path.join(\"LLaVA-OneVision-Data/ureader_qa\", item[\"image\"])\n",
    "    new_json_data.append(item)\n",
    "save_json_path = \"/data_ssd/USER_DATA/omote/iam2/LLaVA-OneVision-Data/ureader_qa/ureader_qa_relative_path.json\"\n",
    "\n",
    "\n",
    "print(len(new_json_data))\n",
    "print(new_json_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab5034e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(new_json_data, save_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8854ab6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37550\n",
      "dict_keys(['id', 'conversations', 'data_source', 'image'])\n",
      "ureader-instruction-1.0/ChartQA/train/png/00006834003065.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37550/37550 [00:07<00:00, 4987.13it/s]\n"
     ]
    }
   ],
   "source": [
    "path = \"/data_ssd/USER_DATA/omote/iam2/LLaVA-OneVision-Data/ureader_kg/ureader_kg_processed.json\"\n",
    "data = load_json(path)\n",
    "print(len(data))\n",
    "print(data[0].keys())\n",
    "print(data[0][\"image\"])\n",
    "\n",
    "image_folder = \"/data_ssd/USER_DATA/omote/iam2/LLaVA-OneVision-Data/ureader_kg\"\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "for item in tqdm(data):\n",
    "    image_path = item[\"image\"]\n",
    "    if not os.path.exists(os.path.join(image_folder, image_path)):\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2b3609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37550\n",
      "{'id': '00006834003065', 'conversations': [{'from': 'human', 'value': '<image>\\nHighlight a few significant elements in this photo.'}, {'from': 'gpt', 'value': 'Nigeria is the country with the longest bar in the world.'}], 'data_source': 'ureader_kg', 'image': 'LLaVA-OneVision-Data/ureader_kg/ureader-instruction-1.0/ChartQA/train/png/00006834003065.png'}\n"
     ]
    }
   ],
   "source": [
    "new_json_data = []\n",
    "\n",
    "for item in data:\n",
    "    item[\"image\"] = os.path.join(\"LLaVA-OneVision-Data/ureader_kg\", item[\"image\"])\n",
    "    new_json_data.append(item)\n",
    "save_json_path = \"/data_ssd/USER_DATA/omote/iam2/LLaVA-OneVision-Data/ureader_kg/ureader_kg_relative_path.json\"\n",
    "\n",
    "\n",
    "print(len(new_json_data))\n",
    "print(new_json_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45cf8039",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(new_json_data, save_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7089a36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

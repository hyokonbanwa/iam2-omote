{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0044d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_json(file_path):\n",
    "    \"\"\"\n",
    "    Load a JSON file and return its content as a Python dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The content of the JSON file as a dictionary.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "def save_json(data, file_path):\n",
    "    \"\"\"\n",
    "    Save a Python dictionary to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): The data to save.\n",
    "        file_path (str): The path where the JSON file will be saved.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd0b2d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "json_root_dir = \"/data_ssd/LLaVA-OneVision-Data\"\n",
    "path_list = glob.glob(os.path.join(json_root_dir, \"*\", \"*_checked_image_tag.json\"), recursive=True)\n",
    "print(len(path_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f82ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_list = [os.path.basename(path).split(\"_checked_image_tag\")[0] for path in path_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb73ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list.append((\"/data_ssd/M4-Instruct-Data/m4_instruct_annotations_fixed.json\"))\n",
    "dataset_name_list.append(\"M4-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17bddfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CLEVR-Math(MathV360K)', 'FigureQA(MathV360K)', 'GEOS(MathV360K)', 'GeoQA+(MathV360K)', 'Geometry3K(MathV360K)', 'IconQA(MathV360K)', 'MapQA(MathV360K)', 'PMC-VQA(MathV360K)', 'Super-CLEVR(MathV360K)', 'TabMWP(MathV360K)', 'UniGeo(MathV360K)', 'VisualWebInstruct(filtered)', 'VizWiz(MathV360K)', 'ai2d(cauldron,llava_format)', 'ai2d(gpt4v)', 'ai2d(internvl)', 'allava_instruct_laion4v', 'allava_instruct_vflan4v', 'aokvqa(cauldron,llava_format)', 'chart2text(cauldron)', 'chartqa(cauldron,llava_format)', 'chrome_writting', 'clevr(cauldron,llava_format)', 'diagram_image_to_text(cauldron)', 'dvqa(cauldron,llava_format)', 'figureqa(cauldron,llava_format)', 'geo170k(align)', 'geo170k(qa)', 'geo3k', 'geomverse(cauldron)', 'hateful_memes(cauldron,llava_format)', 'hitab(cauldron,llava_format)', 'hme100k', 'iam(cauldron)', 'iconqa(cauldron,llava_format)', 'iiit5k', 'image_textualization(filtered)', 'infographic(gpt4v)', 'infographic_vqa', 'infographic_vqa_llava_format', 'intergps(cauldron,llava_format)', 'k12_printing', 'llavar_gpt4_20k', 'lrv_chart', 'lrv_normal(filtered)', 'magpie_pro(l3_80b_mt)', 'magpie_pro(l3_80b_st)', 'magpie_pro(qwen2_72b_st)', 'mapqa(cauldron,llava_format)', 'mathqa', 'mavis_math_metagen', 'mavis_math_rule_geo', 'multihiertt(cauldron)', 'orand_car_a', 'raven(cauldron)', 'rendered_text(cauldron)', 'robut_sqa(cauldron)', 'robut_wikisql(cauldron)', 'robut_wtq(cauldron,llava_format)', 'scienceqa(cauldron,llava_format)', 'scienceqa(nona_context)', 'screen2words(cauldron)', 'sharegpt4o', 'sharegpt4v(coco)', 'sharegpt4v(knowledge)', 'sharegpt4v(llava)', 'sharegpt4v(sam)', 'sroie', 'st_vqa(cauldron,llava_format)', 'tabmwp(cauldron)', 'tallyqa(cauldron,llava_format)', 'textcaps', 'textocr(gpt4v)', 'tqa(cauldron,llava_format)', 'ureader_cap', 'ureader_ie', 'vision_flan(filtered)', 'vistext(cauldron)', 'visual7w(cauldron,llava_format)', 'visualmrc(cauldron)', 'vqarad(cauldron,llava_format)', 'vsr(cauldron,llava_format)', 'websight(cauldron)', 'llava_wild_4v_39k_filtered', 'MathV360K_VQA-RAD', 'MathV360K_VQA-AS', 'Evol-Instruct-GPT4-Turbo', 'llava_wild_4v_12k_filtered', 'MathV360K_TQA', 'ureader_kg', 'ureader_qa', 'M4-Instruct']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [01:50,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_name_list)\n",
    "from tqdm import tqdm\n",
    "dataset_dict = {}\n",
    "total_data_num = 0\n",
    "for dataset_name, path in tqdm(zip(dataset_name_list, path_list)):\n",
    "    data = load_json(path)\n",
    "    dataset_dict[dataset_name] = {\"data\":data, \"data_num\": len(data)}\n",
    "    total_data_num += len(data)\n",
    "    if len(data) == 0:\n",
    "        print(f\"Warning: {dataset_name} has no data, please check the path: {path}\")\n",
    "\n",
    "for dataset_name, dataset_info in dataset_dict.items():\n",
    "    dataset_dict[dataset_name][\"weight\"] = dataset_info[\"data_num\"] / total_data_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5173a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEVR-Math(MathV360K): 5280 samples, weight: 0.0011\n",
      "FigureQA(MathV360K): 17587 samples, weight: 0.0036\n",
      "GEOS(MathV360K): 498 samples, weight: 0.0001\n",
      "GeoQA+(MathV360K): 17162 samples, weight: 0.0035\n",
      "Geometry3K(MathV360K): 9724 samples, weight: 0.0020\n",
      "IconQA(MathV360K): 22589 samples, weight: 0.0047\n",
      "MapQA(MathV360K): 5225 samples, weight: 0.0011\n",
      "PMC-VQA(MathV360K): 35948 samples, weight: 0.0074\n",
      "Super-CLEVR(MathV360K): 8642 samples, weight: 0.0018\n",
      "TabMWP(MathV360K): 22452 samples, weight: 0.0046\n",
      "UniGeo(MathV360K): 11949 samples, weight: 0.0025\n",
      "VisualWebInstruct(filtered): 263584 samples, weight: 0.0543\n",
      "VizWiz(MathV360K): 6604 samples, weight: 0.0014\n",
      "ai2d(cauldron,llava_format): 2429 samples, weight: 0.0005\n",
      "ai2d(gpt4v): 4864 samples, weight: 0.0010\n",
      "ai2d(internvl): 12403 samples, weight: 0.0026\n",
      "allava_instruct_laion4v: 49990 samples, weight: 0.0103\n",
      "allava_instruct_vflan4v: 19990 samples, weight: 0.0041\n",
      "aokvqa(cauldron,llava_format): 16534 samples, weight: 0.0034\n",
      "chart2text(cauldron): 26956 samples, weight: 0.0056\n",
      "chartqa(cauldron,llava_format): 18260 samples, weight: 0.0038\n",
      "chrome_writting: 8825 samples, weight: 0.0018\n",
      "clevr(cauldron,llava_format): 69995 samples, weight: 0.0144\n",
      "diagram_image_to_text(cauldron): 295 samples, weight: 0.0001\n",
      "dvqa(cauldron,llava_format): 199995 samples, weight: 0.0412\n",
      "figureqa(cauldron,llava_format): 99995 samples, weight: 0.0206\n",
      "geo170k(align): 60242 samples, weight: 0.0124\n",
      "geo170k(qa): 67823 samples, weight: 0.0140\n",
      "geo3k: 2091 samples, weight: 0.0004\n",
      "geomverse(cauldron): 9298 samples, weight: 0.0019\n",
      "hateful_memes(cauldron,llava_format): 8495 samples, weight: 0.0018\n",
      "hitab(cauldron,llava_format): 2495 samples, weight: 0.0005\n",
      "hme100k: 74492 samples, weight: 0.0154\n",
      "iam(cauldron): 5658 samples, weight: 0.0012\n",
      "iconqa(cauldron,llava_format): 27302 samples, weight: 0.0056\n",
      "iiit5k: 1990 samples, weight: 0.0004\n",
      "image_textualization(filtered): 99573 samples, weight: 0.0205\n",
      "infographic(gpt4v): 1982 samples, weight: 0.0004\n",
      "infographic_vqa: 4394 samples, weight: 0.0009\n",
      "infographic_vqa_llava_format: 2113 samples, weight: 0.0004\n",
      "intergps(cauldron,llava_format): 1275 samples, weight: 0.0003\n",
      "k12_printing: 256636 samples, weight: 0.0529\n",
      "llavar_gpt4_20k: 19790 samples, weight: 0.0041\n",
      "lrv_chart: 1776 samples, weight: 0.0004\n",
      "lrv_normal(filtered): 10490 samples, weight: 0.0022\n",
      "magpie_pro(l3_80b_mt): 299988 samples, weight: 0.0618\n",
      "magpie_pro(l3_80b_st): 299990 samples, weight: 0.0618\n",
      "magpie_pro(qwen2_72b_st): 299982 samples, weight: 0.0618\n",
      "mapqa(cauldron,llava_format): 37412 samples, weight: 0.0077\n",
      "mathqa: 29827 samples, weight: 0.0061\n",
      "mavis_math_metagen: 87348 samples, weight: 0.0180\n",
      "mavis_math_rule_geo: 99990 samples, weight: 0.0206\n",
      "multihiertt(cauldron): 7614 samples, weight: 0.0016\n",
      "orand_car_a: 1999 samples, weight: 0.0004\n",
      "raven(cauldron): 41995 samples, weight: 0.0087\n",
      "rendered_text(cauldron): 9995 samples, weight: 0.0021\n",
      "robut_sqa(cauldron): 8509 samples, weight: 0.0018\n",
      "robut_wikisql(cauldron): 74984 samples, weight: 0.0155\n",
      "robut_wtq(cauldron,llava_format): 38241 samples, weight: 0.0079\n",
      "scienceqa(cauldron,llava_format): 4971 samples, weight: 0.0010\n",
      "scienceqa(nona_context): 19208 samples, weight: 0.0040\n",
      "screen2words(cauldron): 15725 samples, weight: 0.0032\n",
      "sharegpt4o: 57284 samples, weight: 0.0118\n",
      "sharegpt4v(coco): 50017 samples, weight: 0.0103\n",
      "sharegpt4v(knowledge): 1988 samples, weight: 0.0004\n",
      "sharegpt4v(llava): 29990 samples, weight: 0.0062\n",
      "sharegpt4v(sam): 8990 samples, weight: 0.0019\n",
      "sroie: 33616 samples, weight: 0.0069\n",
      "st_vqa(cauldron,llava_format): 17242 samples, weight: 0.0036\n",
      "tabmwp(cauldron): 22717 samples, weight: 0.0047\n",
      "tallyqa(cauldron,llava_format): 98675 samples, weight: 0.0203\n",
      "textcaps: 21942 samples, weight: 0.0045\n",
      "textocr(gpt4v): 25104 samples, weight: 0.0052\n",
      "tqa(cauldron,llava_format): 27302 samples, weight: 0.0056\n",
      "ureader_cap: 91434 samples, weight: 0.0188\n",
      "ureader_ie: 17322 samples, weight: 0.0036\n",
      "vision_flan(filtered): 186060 samples, weight: 0.0384\n",
      "vistext(cauldron): 9964 samples, weight: 0.0021\n",
      "visual7w(cauldron,llava_format): 14361 samples, weight: 0.0030\n",
      "visualmrc(cauldron): 3022 samples, weight: 0.0006\n",
      "vqarad(cauldron,llava_format): 308 samples, weight: 0.0001\n",
      "vsr(cauldron,llava_format): 2152 samples, weight: 0.0004\n",
      "websight(cauldron): 9995 samples, weight: 0.0021\n",
      "llava_wild_4v_39k_filtered: 39362 samples, weight: 0.0081\n",
      "MathV360K_VQA-RAD: 2125 samples, weight: 0.0004\n",
      "MathV360K_VQA-AS: 5902 samples, weight: 0.0012\n",
      "Evol-Instruct-GPT4-Turbo: 142985 samples, weight: 0.0295\n",
      "llava_wild_4v_12k_filtered: 15124 samples, weight: 0.0031\n",
      "MathV360K_TQA: 10176 samples, weight: 0.0021\n",
      "ureader_kg: 37550 samples, weight: 0.0077\n",
      "ureader_qa: 252954 samples, weight: 0.0521\n",
      "M4-Instruct: 621548 samples, weight: 0.1281\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset_info in dataset_dict.items():\n",
    "    print(f\"{dataset_name}: {dataset_info['data_num']} samples, weight: {dataset_info['weight']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbcee989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4850679\n",
      "min data num: 295\n"
     ]
    }
   ],
   "source": [
    "print(total_data_num)\n",
    "print(f\"min data num: {min([dataset_info['data_num'] for dataset_info in dataset_dict.values()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac2b966",
   "metadata": {},
   "source": [
    "# 最低保証サンプル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7cddb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "from collections import Counter\n",
    "\n",
    "sample_data_num = 20000 #50000 #20000\n",
    "dataset_name_list = []\n",
    "weights = []\n",
    "import numpy as np\n",
    "sample_num_per_dataset = []\n",
    "for dataset_name, dataset_info in dataset_dict.items():\n",
    "    dataset_name_list.append(dataset_name)\n",
    "    sample_num_per_dataset.append(round(dataset_info[\"weight\"] * sample_data_num) if dataset_info[\"weight\"] > 0 else 1 )\n",
    "    \n",
    "min_sample_num = 10 #min(sample_num_per_dataset)\n",
    "\n",
    "for dataset_info in dataset_dict.values():\n",
    "    #weights.append((dataset_info[\"data_num\"]-min_sample_num) / (total_data_num - len(dataset_dict) * min_sample_num))\n",
    "    weights.append(round(dataset_info[\"weight\"] * sample_data_num) - min_sample_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31a289f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19997\n",
      "Minimum sample number per dataset: 10\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(sample_num_per_dataset))\n",
    "print(f\"Minimum sample number per dataset: {min_sample_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9e5c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 920 samples from the datasets.\n"
     ]
    }
   ],
   "source": [
    "sample_num_per_dataset = {k:min_sample_num for k in dataset_dict.keys()}\n",
    "print(f\"Sampled {sum(sample_num_per_dataset.values())} samples from the datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2589115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset_iter = random.choices(dataset_name_list, weights=weights, k=(sample_data_num - len(dataset_name_list) * min_sample_num))\n",
    "sample_num_counter = Counter(sample_dataset_iter)\n",
    "for k, v in sample_num_counter.items():\n",
    "    sample_num_per_dataset[k] += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31ba2aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEVR-Math(MathV360K): 29 samples, target: 22\n",
      "FigureQA(MathV360K): 58 samples, target: 73\n",
      "GEOS(MathV360K): 10 samples, target: 2\n",
      "GeoQA+(MathV360K): 66 samples, target: 71\n",
      "Geometry3K(MathV360K): 37 samples, target: 40\n",
      "IconQA(MathV360K): 89 samples, target: 93\n",
      "MapQA(MathV360K): 18 samples, target: 22\n",
      "PMC-VQA(MathV360K): 131 samples, target: 148\n",
      "Super-CLEVR(MathV360K): 37 samples, target: 36\n",
      "TabMWP(MathV360K): 86 samples, target: 93\n",
      "UniGeo(MathV360K): 49 samples, target: 49\n",
      "VisualWebInstruct(filtered): 1069 samples, target: 1087\n",
      "VizWiz(MathV360K): 32 samples, target: 27\n",
      "ai2d(cauldron,llava_format): 10 samples, target: 10\n",
      "ai2d(gpt4v): 21 samples, target: 20\n",
      "ai2d(internvl): 39 samples, target: 51\n",
      "allava_instruct_laion4v: 231 samples, target: 206\n",
      "allava_instruct_vflan4v: 102 samples, target: 82\n",
      "aokvqa(cauldron,llava_format): 61 samples, target: 68\n",
      "chart2text(cauldron): 116 samples, target: 111\n",
      "chartqa(cauldron,llava_format): 82 samples, target: 75\n",
      "chrome_writting: 36 samples, target: 36\n",
      "clevr(cauldron,llava_format): 292 samples, target: 289\n",
      "diagram_image_to_text(cauldron): 10 samples, target: 1\n",
      "dvqa(cauldron,llava_format): 849 samples, target: 825\n",
      "figureqa(cauldron,llava_format): 444 samples, target: 412\n",
      "geo170k(align): 232 samples, target: 248\n",
      "geo170k(qa): 255 samples, target: 280\n",
      "geo3k: 10 samples, target: 9\n",
      "geomverse(cauldron): 37 samples, target: 38\n",
      "hateful_memes(cauldron,llava_format): 38 samples, target: 35\n",
      "hitab(cauldron,llava_format): 10 samples, target: 10\n",
      "hme100k: 270 samples, target: 307\n",
      "iam(cauldron): 19 samples, target: 23\n",
      "iconqa(cauldron,llava_format): 98 samples, target: 113\n",
      "iiit5k: 10 samples, target: 8\n",
      "image_textualization(filtered): 403 samples, target: 411\n",
      "infographic(gpt4v): 10 samples, target: 8\n",
      "infographic_vqa: 16 samples, target: 18\n",
      "infographic_vqa_llava_format: 10 samples, target: 9\n",
      "intergps(cauldron,llava_format): 10 samples, target: 5\n",
      "k12_printing: 1077 samples, target: 1058\n",
      "llavar_gpt4_20k: 83 samples, target: 82\n",
      "lrv_chart: 10 samples, target: 7\n",
      "lrv_normal(filtered): 46 samples, target: 43\n",
      "magpie_pro(l3_80b_mt): 1221 samples, target: 1237\n",
      "magpie_pro(l3_80b_st): 1207 samples, target: 1237\n",
      "magpie_pro(qwen2_72b_st): 1186 samples, target: 1237\n",
      "mapqa(cauldron,llava_format): 161 samples, target: 154\n",
      "mathqa: 118 samples, target: 123\n",
      "mavis_math_metagen: 368 samples, target: 360\n",
      "mavis_math_rule_geo: 442 samples, target: 412\n",
      "multihiertt(cauldron): 33 samples, target: 31\n",
      "orand_car_a: 10 samples, target: 8\n",
      "raven(cauldron): 173 samples, target: 173\n",
      "rendered_text(cauldron): 40 samples, target: 41\n",
      "robut_sqa(cauldron): 29 samples, target: 35\n",
      "robut_wikisql(cauldron): 312 samples, target: 309\n",
      "robut_wtq(cauldron,llava_format): 176 samples, target: 158\n",
      "scienceqa(cauldron,llava_format): 17 samples, target: 20\n",
      "scienceqa(nona_context): 77 samples, target: 79\n",
      "screen2words(cauldron): 73 samples, target: 65\n",
      "sharegpt4o: 225 samples, target: 236\n",
      "sharegpt4v(coco): 210 samples, target: 206\n",
      "sharegpt4v(knowledge): 10 samples, target: 8\n",
      "sharegpt4v(llava): 128 samples, target: 124\n",
      "sharegpt4v(sam): 36 samples, target: 37\n",
      "sroie: 158 samples, target: 139\n",
      "st_vqa(cauldron,llava_format): 62 samples, target: 71\n",
      "tabmwp(cauldron): 93 samples, target: 94\n",
      "tallyqa(cauldron,llava_format): 411 samples, target: 407\n",
      "textcaps: 91 samples, target: 90\n",
      "textocr(gpt4v): 100 samples, target: 104\n",
      "tqa(cauldron,llava_format): 106 samples, target: 113\n",
      "ureader_cap: 406 samples, target: 377\n",
      "ureader_ie: 61 samples, target: 71\n",
      "vision_flan(filtered): 776 samples, target: 767\n",
      "vistext(cauldron): 41 samples, target: 41\n",
      "visual7w(cauldron,llava_format): 42 samples, target: 59\n",
      "visualmrc(cauldron): 10 samples, target: 12\n",
      "vqarad(cauldron,llava_format): 10 samples, target: 1\n",
      "vsr(cauldron,llava_format): 10 samples, target: 9\n",
      "websight(cauldron): 41 samples, target: 41\n",
      "llava_wild_4v_39k_filtered: 177 samples, target: 162\n",
      "MathV360K_VQA-RAD: 10 samples, target: 9\n",
      "MathV360K_VQA-AS: 22 samples, target: 24\n",
      "Evol-Instruct-GPT4-Turbo: 606 samples, target: 590\n",
      "llava_wild_4v_12k_filtered: 61 samples, target: 62\n",
      "MathV360K_TQA: 33 samples, target: 42\n",
      "ureader_kg: 181 samples, target: 155\n",
      "ureader_qa: 1062 samples, target: 1043\n",
      "M4-Instruct: 2530 samples, target: 2563\n",
      "Total sampled data number: 20000\n"
     ]
    }
   ],
   "source": [
    "for k, v in sample_num_per_dataset.items():\n",
    "    print(f\"{k}: {v} samples, target: {round(dataset_dict[k]['weight'] * sample_data_num)}\")\n",
    "    \n",
    "print(f\"Total sampled data number: {sum(sample_num_per_dataset.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc53e94",
   "metadata": {},
   "source": [
    "# 実際にサンプル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05ff3ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sampled data number: 20000\n"
     ]
    }
   ],
   "source": [
    "save_json_data = []\n",
    "\n",
    "for dataset_name, sample_num in sample_num_per_dataset.items():\n",
    "    data = dataset_dict[dataset_name][\"data\"]\n",
    "    if sample_num > len(data):\n",
    "        print(f\"Warning: {dataset_name} has only {len(data)} samples, but requested {sample_num} samples.\")\n",
    "        sample_num = len(data)\n",
    "    sampled_data = random.sample(data, sample_num)\n",
    "    save_json_data.extend(sampled_data)\n",
    "    \n",
    "print(f\"Total sampled data number: {len(save_json_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8909344",
   "metadata": {},
   "source": [
    "# image数チェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e709be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:07<00:00, 2747.68it/s] \n"
     ]
    }
   ],
   "source": [
    "image_folder_root = \"/data_ssd/llava-onevision-data-symbolic-link\"\n",
    "for item in tqdm(save_json_data):\n",
    "    if \"image\" in item:\n",
    "        image_list = item[\"image\"] if isinstance(item[\"image\"], list) else [item[\"image\"]]\n",
    "        iamge_list = [os.path.join(image_folder_root, img) for img in image_list]\n",
    "        \n",
    "        for image_path in iamge_list:\n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Warning: Image path {image_path} does not exist, removing item from data.\")\n",
    "            \n",
    "        image_count = 0\n",
    "        for conversation in item[\"conversations\"]:\n",
    "            image_count += conversation[\"value\"].count(\"<image>\")\n",
    "            \n",
    "        if image_count != len(image_list):\n",
    "            print(image_list[0])\n",
    "            break\n",
    "            \n",
    "            # print(f\"{item[\"image\"]} has more than one <image> tag {image_count}, removing item from data.\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d8981dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e08fc5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource: nextqa\n",
      "id: 3104055504\n",
      "image: ['nextqa/0.0.0/c55fcb268ead378049e4743c77ca2db3142e12a0f7dfc42eb8267e08efa85f58/train_images/3104055504_0.jpg', 'nextqa/0.0.0/c55fcb268ead378049e4743c77ca2db3142e12a0f7dfc42eb8267e08efa85f58/train_images/3104055504_1.jpg', 'nextqa/0.0.0/c55fcb268ead378049e4743c77ca2db3142e12a0f7dfc42eb8267e08efa85f58/train_images/3104055504_2.jpg', 'nextqa/0.0.0/c55fcb268ead378049e4743c77ca2db3142e12a0f7dfc42eb8267e08efa85f58/train_images/3104055504_3.jpg', 'nextqa/0.0.0/c55fcb268ead378049e4743c77ca2db3142e12a0f7dfc42eb8267e08efa85f58/train_images/3104055504_4.jpg', 'nextqa/0.0.0/c55fcb268ead378049e4743c77ca2db3142e12a0f7dfc42eb8267e08efa85f58/train_images/3104055504_5.jpg', 'nextqa/0.0.0/c55fcb268ead378049e4743c77ca2db3142e12a0f7dfc42eb8267e08efa85f58/train_images/3104055504_6.jpg', 'nextqa/0.0.0/c55fcb268ead378049e4743c77ca2db3142e12a0f7dfc42eb8267e08efa85f58/train_images/3104055504_7.jpg']\n",
      "conversations: [{'from': 'human', 'value': '<image><image><image><image><image><image><image><image>What does the person do to the hamster at the end?\\nA: use spade to take soil\\nB: hop forward\\nC: crawl to adult\\nD: adjust the hamster\\nE: put hand on head\\n'}, {'from': 'gpt', 'value': 'D: adjust the hamster'}, {'from': 'human', 'value': 'How many hamsters are there?\\nA: six\\nB: one\\nC: two\\nD: three\\nE: four\\n'}, {'from': 'gpt', 'value': 'B: one'}, {'from': 'human', 'value': 'Why does the hamster come down from the table?\\nA: table was blocking them\\nB: look for something\\nC: entice the cat\\nD: eager to eat\\nE: pick up the romote controller\\n'}, {'from': 'gpt', 'value': 'B: look for something'}, {'from': 'human', 'value': 'What does the hamster do after sitting up straight in front of the chair?\\nA: adjust himself\\nB: sit up\\nC: cat\\nD: stay still\\nE: adjust the hamster\\n'}, {'from': 'gpt', 'value': 'D: stay still'}, {'from': 'human', 'value': 'Why does the man grab the hamster after it jump down form the table?\\nA: give it to the chickens\\nB: get down from bed\\nC: put it on the table\\nD: to put it somewhere\\nE: not finish the food in mouth\\n'}, {'from': 'gpt', 'value': 'C: put it on the table'}, {'from': 'human', 'value': 'How did the hamster get down from the table?\\nA: jump\\nB: use hands\\nC: using his hands\\nD: man carried it down\\nE: stand on hind legs\\n'}, {'from': 'gpt', 'value': 'A: jump'}, {'from': 'human', 'value': 'How did the man shift the hamster s position?\\nA: pick it up\\nB: put his leg over\\nC: using toy\\nD: steer\\nE: putting it on the table\\n'}, {'from': 'gpt', 'value': 'A: pick it up'}, {'from': 'human', 'value': 'What does the man do after the hamster jumped off the table?\\nA: walk towards the girl\\nB: pick it up\\nC: pick up hamster\\nD: goes back\\nE: run to kitchen\\n'}, {'from': 'gpt', 'value': 'C: pick up hamster'}, {'from': 'human', 'value': 'What does the hamster do after it was transferred to another position by the man?\\nA: grab the back of his chair\\nB: perform stunt again\\nC: sniff around\\nD: get up\\nE: put the baby back\\n'}, {'from': 'gpt', 'value': 'C: sniff around'}]\n",
      "metadata: {'dataset': 'nextqa', 'split': 'train', 'num_sample': 0, 'task_instruction': '', 'question_type': 'multi-choice'}\n"
     ]
    }
   ],
   "source": [
    "for k, v in item.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f49d9ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json_path = os.path.join(\"/data_ssd/LLaVA-OneVision-Data-M4-Instrct-Json\", f\"llava-onevision-m4-instruct_{sample_data_num}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d72721ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(save_json_data, save_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbbb2c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20000 samples from /data_ssd/LLaVA-OneVision-Data-M4-Instrct-Json/llava-onevision-m4-instruct_20000.json.\n"
     ]
    }
   ],
   "source": [
    "loaded_data = load_json(save_json_path)\n",
    "print(f\"Loaded {len(loaded_data)} samples from {save_json_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03e60f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'identity_168600', 'conversations': [{'from': 'human', 'value': '<image>\\nHint: Please answer the question and provide the final answer at the end.\\nQuestion: How many objects are there in total?'}, {'from': 'gpt', 'value': 'The answer is 9'}], 'data_source': 'CLEVR-Math(MathV360K)', 'image': 'LLaVA-OneVision-Data/CLEVR-Math(MathV360K)/train/identity_168600.png'}\n"
     ]
    }
   ],
   "source": [
    "print(loaded_data[0])  # Print the first item to verify the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2459cdc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

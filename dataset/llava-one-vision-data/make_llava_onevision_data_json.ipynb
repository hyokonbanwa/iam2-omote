{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0044d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_json(file_path):\n",
    "    \"\"\"\n",
    "    Load a JSON file and return its content as a Python dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The content of the JSON file as a dictionary.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "def save_json(data, file_path):\n",
    "    \"\"\"\n",
    "    Save a Python dictionary to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): The data to save.\n",
    "        file_path (str): The path where the JSON file will be saved.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd0b2d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "json_root_dir = \"/data_ssd/LLaVA-OneVision-Data\"\n",
    "path_list = glob.glob(os.path.join(json_root_dir, \"*\", \"*_checked_image_tag.json\"), recursive=True)\n",
    "print(len(path_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f82ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_list = [os.path.basename(path).split(\"_checked_image_tag\")[0] for path in path_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17bddfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CLEVR-Math(MathV360K)', 'FigureQA(MathV360K)', 'GEOS(MathV360K)', 'GeoQA+(MathV360K)', 'Geometry3K(MathV360K)', 'IconQA(MathV360K)', 'MapQA(MathV360K)', 'PMC-VQA(MathV360K)', 'Super-CLEVR(MathV360K)', 'TabMWP(MathV360K)', 'UniGeo(MathV360K)', 'VisualWebInstruct(filtered)', 'VizWiz(MathV360K)', 'ai2d(cauldron,llava_format)', 'ai2d(gpt4v)', 'ai2d(internvl)', 'allava_instruct_laion4v', 'allava_instruct_vflan4v', 'aokvqa(cauldron,llava_format)', 'chart2text(cauldron)', 'chartqa(cauldron,llava_format)', 'chrome_writting', 'clevr(cauldron,llava_format)', 'diagram_image_to_text(cauldron)', 'dvqa(cauldron,llava_format)', 'figureqa(cauldron,llava_format)', 'geo170k(align)', 'geo170k(qa)', 'geo3k', 'geomverse(cauldron)', 'hateful_memes(cauldron,llava_format)', 'hitab(cauldron,llava_format)', 'hme100k', 'iam(cauldron)', 'iconqa(cauldron,llava_format)', 'iiit5k', 'image_textualization(filtered)', 'infographic(gpt4v)', 'infographic_vqa', 'infographic_vqa_llava_format', 'intergps(cauldron,llava_format)', 'k12_printing', 'llavar_gpt4_20k', 'lrv_chart', 'lrv_normal(filtered)', 'magpie_pro(l3_80b_mt)', 'magpie_pro(l3_80b_st)', 'magpie_pro(qwen2_72b_st)', 'mapqa(cauldron,llava_format)', 'mathqa', 'mavis_math_metagen', 'mavis_math_rule_geo', 'multihiertt(cauldron)', 'orand_car_a', 'raven(cauldron)', 'rendered_text(cauldron)', 'robut_sqa(cauldron)', 'robut_wikisql(cauldron)', 'robut_wtq(cauldron,llava_format)', 'scienceqa(cauldron,llava_format)', 'scienceqa(nona_context)', 'screen2words(cauldron)', 'sharegpt4o', 'sharegpt4v(coco)', 'sharegpt4v(knowledge)', 'sharegpt4v(llava)', 'sharegpt4v(sam)', 'sroie', 'st_vqa(cauldron,llava_format)', 'tabmwp(cauldron)', 'tallyqa(cauldron,llava_format)', 'textcaps', 'textocr(gpt4v)', 'tqa(cauldron,llava_format)', 'ureader_cap', 'ureader_ie', 'vision_flan(filtered)', 'vistext(cauldron)', 'visual7w(cauldron,llava_format)', 'visualmrc(cauldron)', 'vqarad(cauldron,llava_format)', 'vsr(cauldron,llava_format)', 'websight(cauldron)', 'llava_wild_4v_39k_filtered', 'MathV360K_VQA-RAD', 'MathV360K_VQA-AS', 'Evol-Instruct-GPT4-Turbo', 'llava_wild_4v_12k_filtered', 'MathV360K_TQA', 'ureader_kg', 'ureader_qa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 12.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [01:25,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_name_list)\n",
    "from tqdm import tqdm\n",
    "dataset_dict = {}\n",
    "total_data_num = 0\n",
    "for dataset_name, path in tqdm(zip(dataset_name_list, path_list)):\n",
    "    data = load_json(path)\n",
    "    dataset_dict[dataset_name] = {\"data\":data, \"data_num\": len(data)}\n",
    "    total_data_num += len(data)\n",
    "    if len(data) == 0:\n",
    "        print(f\"Warning: {dataset_name} has no data, please check the path: {path}\")\n",
    "\n",
    "for dataset_name, dataset_info in dataset_dict.items():\n",
    "    dataset_dict[dataset_name][\"weight\"] = dataset_info[\"data_num\"] / total_data_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5173a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEVR-Math(MathV360K): 5280 samples, weight: 0.0012\n",
      "FigureQA(MathV360K): 17587 samples, weight: 0.0042\n",
      "GEOS(MathV360K): 498 samples, weight: 0.0001\n",
      "GeoQA+(MathV360K): 17162 samples, weight: 0.0041\n",
      "Geometry3K(MathV360K): 9724 samples, weight: 0.0023\n",
      "IconQA(MathV360K): 22589 samples, weight: 0.0053\n",
      "MapQA(MathV360K): 5225 samples, weight: 0.0012\n",
      "PMC-VQA(MathV360K): 35948 samples, weight: 0.0085\n",
      "Super-CLEVR(MathV360K): 8642 samples, weight: 0.0020\n",
      "TabMWP(MathV360K): 22452 samples, weight: 0.0053\n",
      "UniGeo(MathV360K): 11949 samples, weight: 0.0028\n",
      "VisualWebInstruct(filtered): 263584 samples, weight: 0.0623\n",
      "VizWiz(MathV360K): 6604 samples, weight: 0.0016\n",
      "ai2d(cauldron,llava_format): 2429 samples, weight: 0.0006\n",
      "ai2d(gpt4v): 4864 samples, weight: 0.0012\n",
      "ai2d(internvl): 12403 samples, weight: 0.0029\n",
      "allava_instruct_laion4v: 49990 samples, weight: 0.0118\n",
      "allava_instruct_vflan4v: 19990 samples, weight: 0.0047\n",
      "aokvqa(cauldron,llava_format): 16534 samples, weight: 0.0039\n",
      "chart2text(cauldron): 26956 samples, weight: 0.0064\n",
      "chartqa(cauldron,llava_format): 18260 samples, weight: 0.0043\n",
      "chrome_writting: 8825 samples, weight: 0.0021\n",
      "clevr(cauldron,llava_format): 69995 samples, weight: 0.0166\n",
      "diagram_image_to_text(cauldron): 295 samples, weight: 0.0001\n",
      "dvqa(cauldron,llava_format): 199995 samples, weight: 0.0473\n",
      "figureqa(cauldron,llava_format): 99995 samples, weight: 0.0236\n",
      "geo170k(align): 60242 samples, weight: 0.0142\n",
      "geo170k(qa): 67823 samples, weight: 0.0160\n",
      "geo3k: 2091 samples, weight: 0.0005\n",
      "geomverse(cauldron): 9298 samples, weight: 0.0022\n",
      "hateful_memes(cauldron,llava_format): 8495 samples, weight: 0.0020\n",
      "hitab(cauldron,llava_format): 2495 samples, weight: 0.0006\n",
      "hme100k: 74492 samples, weight: 0.0176\n",
      "iam(cauldron): 5658 samples, weight: 0.0013\n",
      "iconqa(cauldron,llava_format): 27302 samples, weight: 0.0065\n",
      "iiit5k: 1990 samples, weight: 0.0005\n",
      "image_textualization(filtered): 99573 samples, weight: 0.0235\n",
      "infographic(gpt4v): 1982 samples, weight: 0.0005\n",
      "infographic_vqa: 4394 samples, weight: 0.0010\n",
      "infographic_vqa_llava_format: 2113 samples, weight: 0.0005\n",
      "intergps(cauldron,llava_format): 1275 samples, weight: 0.0003\n",
      "k12_printing: 256636 samples, weight: 0.0607\n",
      "llavar_gpt4_20k: 19790 samples, weight: 0.0047\n",
      "lrv_chart: 1776 samples, weight: 0.0004\n",
      "lrv_normal(filtered): 10490 samples, weight: 0.0025\n",
      "magpie_pro(l3_80b_mt): 299988 samples, weight: 0.0709\n",
      "magpie_pro(l3_80b_st): 299990 samples, weight: 0.0709\n",
      "magpie_pro(qwen2_72b_st): 299982 samples, weight: 0.0709\n",
      "mapqa(cauldron,llava_format): 37412 samples, weight: 0.0088\n",
      "mathqa: 29827 samples, weight: 0.0071\n",
      "mavis_math_metagen: 87348 samples, weight: 0.0207\n",
      "mavis_math_rule_geo: 99990 samples, weight: 0.0236\n",
      "multihiertt(cauldron): 7614 samples, weight: 0.0018\n",
      "orand_car_a: 1999 samples, weight: 0.0005\n",
      "raven(cauldron): 41995 samples, weight: 0.0099\n",
      "rendered_text(cauldron): 9995 samples, weight: 0.0024\n",
      "robut_sqa(cauldron): 8509 samples, weight: 0.0020\n",
      "robut_wikisql(cauldron): 74984 samples, weight: 0.0177\n",
      "robut_wtq(cauldron,llava_format): 38241 samples, weight: 0.0090\n",
      "scienceqa(cauldron,llava_format): 4971 samples, weight: 0.0012\n",
      "scienceqa(nona_context): 19208 samples, weight: 0.0045\n",
      "screen2words(cauldron): 15725 samples, weight: 0.0037\n",
      "sharegpt4o: 57284 samples, weight: 0.0135\n",
      "sharegpt4v(coco): 50017 samples, weight: 0.0118\n",
      "sharegpt4v(knowledge): 1988 samples, weight: 0.0005\n",
      "sharegpt4v(llava): 29990 samples, weight: 0.0071\n",
      "sharegpt4v(sam): 8990 samples, weight: 0.0021\n",
      "sroie: 33616 samples, weight: 0.0079\n",
      "st_vqa(cauldron,llava_format): 17242 samples, weight: 0.0041\n",
      "tabmwp(cauldron): 22717 samples, weight: 0.0054\n",
      "tallyqa(cauldron,llava_format): 98675 samples, weight: 0.0233\n",
      "textcaps: 21942 samples, weight: 0.0052\n",
      "textocr(gpt4v): 25104 samples, weight: 0.0059\n",
      "tqa(cauldron,llava_format): 27302 samples, weight: 0.0065\n",
      "ureader_cap: 91434 samples, weight: 0.0216\n",
      "ureader_ie: 17322 samples, weight: 0.0041\n",
      "vision_flan(filtered): 186060 samples, weight: 0.0440\n",
      "vistext(cauldron): 9964 samples, weight: 0.0024\n",
      "visual7w(cauldron,llava_format): 14361 samples, weight: 0.0034\n",
      "visualmrc(cauldron): 3022 samples, weight: 0.0007\n",
      "vqarad(cauldron,llava_format): 308 samples, weight: 0.0001\n",
      "vsr(cauldron,llava_format): 2152 samples, weight: 0.0005\n",
      "websight(cauldron): 9995 samples, weight: 0.0024\n",
      "llava_wild_4v_39k_filtered: 39362 samples, weight: 0.0093\n",
      "MathV360K_VQA-RAD: 2125 samples, weight: 0.0005\n",
      "MathV360K_VQA-AS: 5902 samples, weight: 0.0014\n",
      "Evol-Instruct-GPT4-Turbo: 142985 samples, weight: 0.0338\n",
      "llava_wild_4v_12k_filtered: 15124 samples, weight: 0.0036\n",
      "MathV360K_TQA: 10176 samples, weight: 0.0024\n",
      "ureader_kg: 37550 samples, weight: 0.0089\n",
      "ureader_qa: 252954 samples, weight: 0.0598\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset_info in dataset_dict.items():\n",
    "    print(f\"{dataset_name}: {dataset_info['data_num']} samples, weight: {dataset_info['weight']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbcee989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4229131\n",
      "min data num: 295\n"
     ]
    }
   ],
   "source": [
    "print(total_data_num)\n",
    "print(f\"min data num: {min([dataset_info['data_num'] for dataset_info in dataset_dict.values()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4411aa12",
   "metadata": {},
   "source": [
    "# 単なる重み付サンプル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f99f2fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_num = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fd6753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "from collections import Counter\n",
    "\n",
    "dataset_name_list = []\n",
    "weights = []\n",
    "for dataset_name, dataset_info in dataset_dict.items():\n",
    "    dataset_name_list.append(dataset_name)\n",
    "    weights.append(dataset_info[\"weight\"])\n",
    "    \n",
    "sample_dataset_iter = random.choices(dataset_name_list, weights=weights, k=sample_data_num)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56df505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 200000 samples from the datasets.\n",
      "91\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sampled {len(sample_dataset_iter)} samples from the datasets.\")\n",
    "print(len(set(sample_dataset_iter)))\n",
    "\n",
    "\n",
    "sample_num_counter = Counter(sample_dataset_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1004126c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "magpie_pro(qwen2_72b_st): 14237 samples, target: 14186\n",
      "magpie_pro(l3_80b_st): 14185 samples, target: 14187\n",
      "magpie_pro(l3_80b_mt): 14044 samples, target: 14187\n",
      "VisualWebInstruct(filtered): 12530 samples, target: 12465\n",
      "ureader_qa: 12100 samples, target: 11962\n",
      "k12_printing: 11993 samples, target: 12137\n",
      "dvqa(cauldron,llava_format): 9442 samples, target: 9458\n",
      "vision_flan(filtered): 8753 samples, target: 8799\n",
      "Evol-Instruct-GPT4-Turbo: 6702 samples, target: 6762\n",
      "mavis_math_rule_geo: 4752 samples, target: 4729\n",
      "figureqa(cauldron,llava_format): 4719 samples, target: 4729\n",
      "image_textualization(filtered): 4672 samples, target: 4709\n",
      "tallyqa(cauldron,llava_format): 4662 samples, target: 4666\n",
      "ureader_cap: 4500 samples, target: 4324\n",
      "mavis_math_metagen: 4190 samples, target: 4131\n",
      "robut_wikisql(cauldron): 3500 samples, target: 3546\n",
      "hme100k: 3498 samples, target: 3523\n",
      "clevr(cauldron,llava_format): 3370 samples, target: 3310\n",
      "geo170k(qa): 3209 samples, target: 3207\n",
      "geo170k(align): 2767 samples, target: 2849\n",
      "sharegpt4o: 2667 samples, target: 2709\n",
      "allava_instruct_laion4v: 2358 samples, target: 2364\n",
      "sharegpt4v(coco): 2325 samples, target: 2365\n",
      "raven(cauldron): 2008 samples, target: 1986\n",
      "robut_wtq(cauldron,llava_format): 1892 samples, target: 1808\n",
      "llava_wild_4v_39k_filtered: 1833 samples, target: 1861\n",
      "ureader_kg: 1753 samples, target: 1776\n",
      "PMC-VQA(MathV360K): 1719 samples, target: 1700\n",
      "mapqa(cauldron,llava_format): 1701 samples, target: 1769\n",
      "sroie: 1556 samples, target: 1590\n",
      "mathqa: 1454 samples, target: 1411\n",
      "sharegpt4v(llava): 1433 samples, target: 1418\n",
      "tqa(cauldron,llava_format): 1358 samples, target: 1291\n",
      "chart2text(cauldron): 1299 samples, target: 1275\n",
      "iconqa(cauldron,llava_format): 1299 samples, target: 1291\n",
      "textocr(gpt4v): 1262 samples, target: 1187\n",
      "TabMWP(MathV360K): 1101 samples, target: 1062\n",
      "tabmwp(cauldron): 1099 samples, target: 1074\n",
      "IconQA(MathV360K): 1040 samples, target: 1068\n",
      "textcaps: 1026 samples, target: 1038\n",
      "allava_instruct_vflan4v: 973 samples, target: 945\n",
      "llavar_gpt4_20k: 911 samples, target: 936\n",
      "chartqa(cauldron,llava_format): 862 samples, target: 864\n",
      "scienceqa(nona_context): 856 samples, target: 908\n",
      "FigureQA(MathV360K): 834 samples, target: 832\n",
      "ureader_ie: 827 samples, target: 819\n",
      "st_vqa(cauldron,llava_format): 813 samples, target: 815\n",
      "GeoQA+(MathV360K): 789 samples, target: 812\n",
      "aokvqa(cauldron,llava_format): 747 samples, target: 782\n",
      "visual7w(cauldron,llava_format): 703 samples, target: 679\n",
      "llava_wild_4v_12k_filtered: 697 samples, target: 715\n",
      "screen2words(cauldron): 695 samples, target: 744\n",
      "ai2d(internvl): 590 samples, target: 587\n",
      "UniGeo(MathV360K): 565 samples, target: 565\n",
      "lrv_normal(filtered): 522 samples, target: 496\n",
      "MathV360K_TQA: 500 samples, target: 481\n",
      "websight(cauldron): 491 samples, target: 473\n",
      "Geometry3K(MathV360K): 472 samples, target: 460\n",
      "chrome_writting: 445 samples, target: 417\n",
      "rendered_text(cauldron): 439 samples, target: 473\n",
      "vistext(cauldron): 431 samples, target: 471\n",
      "hateful_memes(cauldron,llava_format): 426 samples, target: 402\n",
      "geomverse(cauldron): 417 samples, target: 440\n",
      "sharegpt4v(sam): 413 samples, target: 425\n",
      "robut_sqa(cauldron): 412 samples, target: 402\n",
      "Super-CLEVR(MathV360K): 407 samples, target: 409\n",
      "multihiertt(cauldron): 360 samples, target: 360\n",
      "VizWiz(MathV360K): 340 samples, target: 312\n",
      "MathV360K_VQA-AS: 273 samples, target: 279\n",
      "iam(cauldron): 268 samples, target: 268\n",
      "ai2d(gpt4v): 262 samples, target: 230\n",
      "scienceqa(cauldron,llava_format): 235 samples, target: 235\n",
      "CLEVR-Math(MathV360K): 231 samples, target: 250\n",
      "infographic_vqa: 217 samples, target: 208\n",
      "MapQA(MathV360K): 205 samples, target: 247\n",
      "visualmrc(cauldron): 149 samples, target: 143\n",
      "ai2d(cauldron,llava_format): 124 samples, target: 115\n",
      "hitab(cauldron,llava_format): 115 samples, target: 118\n",
      "infographic_vqa_llava_format: 110 samples, target: 100\n",
      "MathV360K_VQA-RAD: 110 samples, target: 100\n",
      "infographic(gpt4v): 99 samples, target: 94\n",
      "geo3k: 95 samples, target: 99\n",
      "iiit5k: 94 samples, target: 94\n",
      "orand_car_a: 94 samples, target: 95\n",
      "sharegpt4v(knowledge): 94 samples, target: 94\n",
      "vsr(cauldron,llava_format): 88 samples, target: 102\n",
      "lrv_chart: 74 samples, target: 84\n",
      "intergps(cauldron,llava_format): 72 samples, target: 60\n",
      "GEOS(MathV360K): 19 samples, target: 24\n",
      "vqarad(cauldron,llava_format): 16 samples, target: 15\n",
      "diagram_image_to_text(cauldron): 11 samples, target: 14\n"
     ]
    }
   ],
   "source": [
    "sample_num_dict = {k:{\"num\":v, \"target_sample_num\": round(dataset_dict[k][\"weight\"]*sample_data_num)} for k, v in sample_num_counter.most_common()}\n",
    "for k, v in sample_num_dict.items():\n",
    "    print(f\"{k}: {v['num']} samples, target: {v['target_sample_num']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac2b966",
   "metadata": {},
   "source": [
    "# 最低保証サンプル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7cddb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "from collections import Counter\n",
    "\n",
    "sample_data_num = 200000 #50000 #20000\n",
    "dataset_name_list = []\n",
    "weights = []\n",
    "import numpy as np\n",
    "sample_num_per_dataset = []\n",
    "for dataset_name, dataset_info in dataset_dict.items():\n",
    "    dataset_name_list.append(dataset_name)\n",
    "    sample_num_per_dataset.append(round(dataset_info[\"weight\"] * sample_data_num) if dataset_info[\"weight\"] > 0 else 1 )\n",
    "    \n",
    "min_sample_num = 10 #min(sample_num_per_dataset)\n",
    "\n",
    "for dataset_info in dataset_dict.values():\n",
    "    #weights.append((dataset_info[\"data_num\"]-min_sample_num) / (total_data_num - len(dataset_dict) * min_sample_num))\n",
    "    weights.append(round(dataset_info[\"weight\"] * sample_data_num) - min_sample_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31a289f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200001\n",
      "Minimum sample number per dataset: 10\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(sample_num_per_dataset))\n",
    "print(f\"Minimum sample number per dataset: {min_sample_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9e5c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 910 samples from the datasets.\n"
     ]
    }
   ],
   "source": [
    "sample_num_per_dataset = {k:min_sample_num for k in dataset_dict.keys()}\n",
    "print(f\"Sampled {sum(sample_num_per_dataset.values())} samples from the datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2589115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset_iter = random.choices(dataset_name_list, weights=weights, k=(sample_data_num - len(dataset_name_list) * min_sample_num))\n",
    "sample_num_counter = Counter(sample_dataset_iter)\n",
    "for k, v in sample_num_counter.items():\n",
    "    sample_num_per_dataset[k] += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31ba2aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEVR-Math(MathV360K): 233 samples, target: 250\n",
      "FigureQA(MathV360K): 840 samples, target: 832\n",
      "GEOS(MathV360K): 22 samples, target: 24\n",
      "GeoQA+(MathV360K): 795 samples, target: 812\n",
      "Geometry3K(MathV360K): 460 samples, target: 460\n",
      "IconQA(MathV360K): 1049 samples, target: 1068\n",
      "MapQA(MathV360K): 214 samples, target: 247\n",
      "PMC-VQA(MathV360K): 1721 samples, target: 1700\n",
      "Super-CLEVR(MathV360K): 369 samples, target: 409\n",
      "TabMWP(MathV360K): 1135 samples, target: 1062\n",
      "UniGeo(MathV360K): 544 samples, target: 565\n",
      "VisualWebInstruct(filtered): 12540 samples, target: 12465\n",
      "VizWiz(MathV360K): 343 samples, target: 312\n",
      "ai2d(cauldron,llava_format): 132 samples, target: 115\n",
      "ai2d(gpt4v): 240 samples, target: 230\n",
      "ai2d(internvl): 613 samples, target: 587\n",
      "allava_instruct_laion4v: 2338 samples, target: 2364\n",
      "allava_instruct_vflan4v: 991 samples, target: 945\n",
      "aokvqa(cauldron,llava_format): 754 samples, target: 782\n",
      "chart2text(cauldron): 1288 samples, target: 1275\n",
      "chartqa(cauldron,llava_format): 875 samples, target: 864\n",
      "chrome_writting: 418 samples, target: 417\n",
      "clevr(cauldron,llava_format): 3391 samples, target: 3310\n",
      "diagram_image_to_text(cauldron): 13 samples, target: 14\n",
      "dvqa(cauldron,llava_format): 9424 samples, target: 9458\n",
      "figureqa(cauldron,llava_format): 4728 samples, target: 4729\n",
      "geo170k(align): 2768 samples, target: 2849\n",
      "geo170k(qa): 3213 samples, target: 3207\n",
      "geo3k: 92 samples, target: 99\n",
      "geomverse(cauldron): 423 samples, target: 440\n",
      "hateful_memes(cauldron,llava_format): 407 samples, target: 402\n",
      "hitab(cauldron,llava_format): 120 samples, target: 118\n",
      "hme100k: 3509 samples, target: 3523\n",
      "iam(cauldron): 274 samples, target: 268\n",
      "iconqa(cauldron,llava_format): 1305 samples, target: 1291\n",
      "iiit5k: 88 samples, target: 94\n",
      "image_textualization(filtered): 4668 samples, target: 4709\n",
      "infographic(gpt4v): 88 samples, target: 94\n",
      "infographic_vqa: 237 samples, target: 208\n",
      "infographic_vqa_llava_format: 86 samples, target: 100\n",
      "intergps(cauldron,llava_format): 65 samples, target: 60\n",
      "k12_printing: 12014 samples, target: 12137\n",
      "llavar_gpt4_20k: 891 samples, target: 936\n",
      "lrv_chart: 101 samples, target: 84\n",
      "lrv_normal(filtered): 506 samples, target: 496\n",
      "magpie_pro(l3_80b_mt): 14056 samples, target: 14187\n",
      "magpie_pro(l3_80b_st): 14183 samples, target: 14187\n",
      "magpie_pro(qwen2_72b_st): 14235 samples, target: 14186\n",
      "mapqa(cauldron,llava_format): 1681 samples, target: 1769\n",
      "mathqa: 1446 samples, target: 1411\n",
      "mavis_math_metagen: 4207 samples, target: 4131\n",
      "mavis_math_rule_geo: 4755 samples, target: 4729\n",
      "multihiertt(cauldron): 343 samples, target: 360\n",
      "orand_car_a: 105 samples, target: 95\n",
      "raven(cauldron): 1997 samples, target: 1986\n",
      "rendered_text(cauldron): 441 samples, target: 473\n",
      "robut_sqa(cauldron): 416 samples, target: 402\n",
      "robut_wikisql(cauldron): 3496 samples, target: 3546\n",
      "robut_wtq(cauldron,llava_format): 1884 samples, target: 1808\n",
      "scienceqa(cauldron,llava_format): 239 samples, target: 235\n",
      "scienceqa(nona_context): 865 samples, target: 908\n",
      "screen2words(cauldron): 692 samples, target: 744\n",
      "sharegpt4o: 2666 samples, target: 2709\n",
      "sharegpt4v(coco): 2326 samples, target: 2365\n",
      "sharegpt4v(knowledge): 98 samples, target: 94\n",
      "sharegpt4v(llava): 1427 samples, target: 1418\n",
      "sharegpt4v(sam): 418 samples, target: 425\n",
      "sroie: 1556 samples, target: 1590\n",
      "st_vqa(cauldron,llava_format): 816 samples, target: 815\n",
      "tabmwp(cauldron): 1100 samples, target: 1074\n",
      "tallyqa(cauldron,llava_format): 4659 samples, target: 4666\n",
      "textcaps: 1021 samples, target: 1038\n",
      "textocr(gpt4v): 1262 samples, target: 1187\n",
      "tqa(cauldron,llava_format): 1353 samples, target: 1291\n",
      "ureader_cap: 4505 samples, target: 4324\n",
      "ureader_ie: 824 samples, target: 819\n",
      "vision_flan(filtered): 8763 samples, target: 8799\n",
      "vistext(cauldron): 431 samples, target: 471\n",
      "visual7w(cauldron,llava_format): 697 samples, target: 679\n",
      "visualmrc(cauldron): 151 samples, target: 143\n",
      "vqarad(cauldron,llava_format): 16 samples, target: 15\n",
      "vsr(cauldron,llava_format): 94 samples, target: 102\n",
      "websight(cauldron): 485 samples, target: 473\n",
      "llava_wild_4v_39k_filtered: 1832 samples, target: 1861\n",
      "MathV360K_VQA-RAD: 106 samples, target: 100\n",
      "MathV360K_VQA-AS: 287 samples, target: 279\n",
      "Evol-Instruct-GPT4-Turbo: 6680 samples, target: 6762\n",
      "llava_wild_4v_12k_filtered: 695 samples, target: 715\n",
      "MathV360K_TQA: 500 samples, target: 481\n",
      "ureader_kg: 1757 samples, target: 1776\n",
      "ureader_qa: 12109 samples, target: 11962\n",
      "Total sampled data number: 200000\n"
     ]
    }
   ],
   "source": [
    "for k, v in sample_num_per_dataset.items():\n",
    "    print(f\"{k}: {v} samples, target: {round(dataset_dict[k]['weight'] * sample_data_num)}\")\n",
    "    \n",
    "print(f\"Total sampled data number: {sum(sample_num_per_dataset.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc53e94",
   "metadata": {},
   "source": [
    "# 実際にサンプル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05ff3ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sampled data number: 200000\n"
     ]
    }
   ],
   "source": [
    "save_json_data = []\n",
    "\n",
    "for dataset_name, sample_num in sample_num_per_dataset.items():\n",
    "    data = dataset_dict[dataset_name][\"data\"]\n",
    "    if sample_num > len(data):\n",
    "        print(f\"Warning: {dataset_name} has only {len(data)} samples, but requested {sample_num} samples.\")\n",
    "        sample_num = len(data)\n",
    "    sampled_data = random.sample(data, sample_num)\n",
    "    save_json_data.extend(sampled_data)\n",
    "    \n",
    "print(f\"Total sampled data number: {len(save_json_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e709be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:19<00:00, 10020.50it/s] \n"
     ]
    }
   ],
   "source": [
    "image_folder_root = \"/data_ssd/llava-onevision-data-symbolic-link\"\n",
    "for item in tqdm(save_json_data):\n",
    "    if \"image\" in item:\n",
    "        image_list = item[\"image\"] if isinstance(item[\"image\"], list) else [item[\"image\"]]\n",
    "        iamge_list = [os.path.join(image_folder_root, img) for img in image_list]\n",
    "        \n",
    "        for image_path in iamge_list:\n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Warning: Image path {image_path} does not exist, removing item from data.\")\n",
    "            \n",
    "        image_count = 0\n",
    "        for conversation in item[\"conversations\"]:\n",
    "            image_count += conversation[\"value\"].count(\"<image>\")\n",
    "            \n",
    "        if image_count != len(image_list):\n",
    "            print(image_list[0])\n",
    "            break\n",
    "            \n",
    "            # print(f\"{item[\"image\"]} has more than one <image> tag {image_count}, removing item from data.\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d8981dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e08fc5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 2-16406152-2\n",
      "conversations: [{'from': 'human', 'value': '<image>\\ninte award receive only 2 nomination, Yes or No?\\nAnswer the question using a single word or phrase.'}, {'from': 'gpt', 'value': 'No'}]\n",
      "data_source: ureader_qa\n",
      "image: LLaVA-OneVision-Data/ureader_qa/ureader-instruction-1.0/DUE_Benchmark/TabFact/pngs/2-16406152-2.png\n"
     ]
    }
   ],
   "source": [
    "for k, v in item.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f49d9ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json_path = os.path.join(\"/data_ssd/LLaVA-OneVision-Data-Json\", f\"llava-onevision-data_single-image_data_{sample_data_num}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d72721ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(save_json_data, save_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbbb2c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200000 samples from /data_ssd/LLaVA-OneVision-Data-Json/llava-onevision-data_single-image_data_200000.json.\n"
     ]
    }
   ],
   "source": [
    "loaded_data = load_json(save_json_path)\n",
    "print(f\"Loaded {len(loaded_data)} samples from {save_json_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03e60f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'identity_97897', 'conversations': [{'from': 'human', 'value': '<image>\\nHint: Please answer the question and provide the final answer at the end.\\nQuestion: How many cylinders are there in total?'}, {'from': 'gpt', 'value': 'The answer is 4'}], 'data_source': 'CLEVR-Math(MathV360K)', 'image': 'LLaVA-OneVision-Data/CLEVR-Math(MathV360K)/train/identity_97897.png'}\n"
     ]
    }
   ],
   "source": [
    "print(loaded_data[0])  # Print the first item to verify the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2459cdc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

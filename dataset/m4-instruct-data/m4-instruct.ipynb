{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f86fca06",
   "metadata": {},
   "source": [
    "È†ÜÂ∫èÂÖ•„ÇåÊõø„Åà„Éá„Éº„Çø\n",
    "ÁîªÂÉè„ÅÆÊûöÊï∞„Åå1Êûö\n",
    "ÂãïÁîª„Åã„Å©„ÅÜ„Åã\n",
    "Ë™ø„Åπ„Å¶‰Ωø„Åà„Çãjson‰ΩúÊàê\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f1f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee6da7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def load_json(file_path):\n",
    "    \"\"\"\n",
    "    Load a JSON file and return its content as a Python dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The content of the JSON file as a dictionary.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e73a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data_ssd/huggingface_dataset/lmms-lab/M4-Instruct-Data/m4_instruct_annotations.json\"\n",
    "path = \"/data_ssd/M4-Instruct-Data/m4_instruct_annotations_max_len_14000.json\"\n",
    "data = load_json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7328131c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397781\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6aeb917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sample_id': 5390, 'conversations': [{'from': 'human', 'value': \"<image><image>\\nWhat's the detailed difference between the 2 images? Please list in detail.\"}, {'from': 'gpt', 'value': 'The differences between the two images are:\\n\\n1. In the second image, there are leaves falling from the sunflowers and the surrounding foliage.\\n2. The ground in the second image is covered with a layer of fallen leaves, adding a carpet-like appearance.'}], 'image': ['HQ-Edit/images/83425.jpg', 'HQ-Edit/images/83426.jpg'], 'choice_list': None, 'metadata': {'dataset': 'HQ-Edit-Diff', 'split': 'train', 'num_sample': 98675, 'task_instruction': \"What's the difference between 2 images?\", 'question_type': 'open-ended'}}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0842078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dict_keys_and_items(dictionary):\n",
    "    \"\"\"\n",
    "    Display the keys and items of a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        dictionary (dict): The dictionary to display.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for key, value in dictionary.items():\n",
    "        print(f\"Key: {key}, Value: {value}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db44899e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: sample_id, Value: 5390\n",
      "Key: conversations, Value: [{'from': 'human', 'value': \"<image><image>\\nWhat's the detailed difference between the 2 images? Please list in detail.\"}, {'from': 'gpt', 'value': 'The differences between the two images are:\\n\\n1. In the second image, there are leaves falling from the sunflowers and the surrounding foliage.\\n2. The ground in the second image is covered with a layer of fallen leaves, adding a carpet-like appearance.'}]\n",
      "Key: image, Value: ['HQ-Edit/images/83425.jpg', 'HQ-Edit/images/83426.jpg']\n",
      "Key: choice_list, Value: None\n",
      "Key: metadata, Value: {'dataset': 'HQ-Edit-Diff', 'split': 'train', 'num_sample': 98675, 'task_instruction': \"What's the difference between 2 images?\", 'question_type': 'open-ended'}\n"
     ]
    }
   ],
   "source": [
    "display_dict_keys_and_items(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18bba1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 140/601614 [00:00<14:22, 697.65it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 601614/601614 [06:06<00:00, 1639.34it/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "image_root_dir = \"/home/omote/local-share-data_ssd/huggingface_dataset/lmms-lab/M4-Instruct-Data\"\n",
    "\n",
    "break_flag = False\n",
    "chcek_data = data[14200:]\n",
    "for d in tqdm(chcek_data):\n",
    "    if type(d[\"image\"]) == str:\n",
    "        image_path_list = [d[\"image\"]]\n",
    "    elif type(d[\"image\"]) == list:\n",
    "        image_path_list = d[\"image\"]\n",
    "        \n",
    "    for image_path in image_path_list:\n",
    "        dataset_name = image_path.split(\"/\")[0]\n",
    "        image_name = image_path[len(dataset_name)+1:]\n",
    "        image_path = os.path.join(image_root_dir,dataset_name,dataset_name,image_name)\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image path does not exist: {image_path}\")\n",
    "            break_flag = True\n",
    "            break\n",
    "    if break_flag:\n",
    "        break\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edffb1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'from': 'human', 'value': \"<image><image>\\nWhat's the detailed difference between the 2 images? Please list in detail.\"}, {'from': 'gpt', 'value': 'The differences between the two images are:\\n\\n1. In the second image, there are leaves falling from the sunflowers and the surrounding foliage.\\n2. The ground in the second image is covered with a layer of fallen leaves, adding a carpet-like appearance.'}]\n"
     ]
    }
   ],
   "source": [
    "print(data[0][\"conversations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2feb7544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_conversation_format(item):\n",
    "    conversation = item[\"conversations\"]\n",
    "    if conversation[0][\"from\"] != \"human\":\n",
    "        return False\n",
    "    \n",
    "    if conversation[-1][\"from\"] != \"gpt\":\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def check_multiple_images(item):\n",
    "    if type(item[\"image\"]) == list and len(item[\"image\"]) > 1:\n",
    "        return True\n",
    "    return False\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09390cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 615814/615814 [00:00<00:00, 1060753.37it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "pass_data_list = []\n",
    "not_pass_data_list = []\n",
    "for i, d in enumerate(tqdm(data)):\n",
    "    if not check_conversation_format(d) or not check_multiple_images(d):\n",
    "        not_pass_data_list.append(d)\n",
    "        continue\n",
    "    \n",
    "    pass_data_list.append(d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5679695f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All image paths exist.\n",
      "Total data: 615814\n",
      "Pass data: 610080\n",
      "Not pass data: 5734\n"
     ]
    }
   ],
   "source": [
    "print(\"All image paths exist.\")\n",
    "print(f\"Total data: {len(data)}\")\n",
    "print(f\"Pass data: {len(pass_data_list)}\")\n",
    "print(f\"Not pass data: {len(not_pass_data_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30901b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(not_pass_data_list[0][\"conversations\"][0][\"value\"].count(\"<image\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b9c220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "def check_image_number(item):\n",
    "    conversation = item[\"conversations\"]\n",
    "    image_count = 0\n",
    "\n",
    "    for message in conversation:\n",
    "        image_count += message[\"value\"].count(\"<image>\")\n",
    "\n",
    "    if image_count != len(item[\"image\"]):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "def check_image_in_human_conversation(item):\n",
    "    conversation = item[\"conversations\"]\n",
    "    for message in conversation:            \n",
    "        if \"<image>\" in message[\"value\"] and message[\"from\"] == \"gpt\":\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d43fd4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5734/5734 [00:00<00:00, 1422075.40it/s]\n"
     ]
    }
   ],
   "source": [
    "not_conversation_list = []\n",
    "not_multiple_images_list = []\n",
    "\n",
    "for d in tqdm(not_pass_data_list):\n",
    "    if not check_image_in_human_conversation(d):\n",
    "        not_conversation_list.append(d)\n",
    "        \n",
    "        continue\n",
    "    if not check_image_number(d) and not check_multiple_images(d):\n",
    "        not_multiple_images_list.append(d)\n",
    "        continue\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a925a8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5734\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(not_conversation_list))\n",
    "print(len(not_multiple_images_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f54bb0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'twitter_post', 'id': 0, 'conversations': [{'from': 'gpt', 'value': 'Help me write a Twitter post considering the following images.\\n<image><image><image><image><image><image><image><image><image>'}, {'from': 'human', 'value': '\"Embracing the serenity of island life where the water is as clear as the skies. üåä‚òÄÔ∏è #IslandVibes #CrystalClear #BeachDays #TravelDiaries\"'}], 'image': ['mmchat/images/mw2048_832851e1gy1foxoneylnaj22c02c07wi.jpg', 'mmchat/images/mw2048_832851e1gy1foxop6mkvgj22av2av7wi.jpg', 'mmchat/images/mw2048_832851e1gy1foxoo3z9v0j2276276hdu.jpg', 'mmchat/images/mw2048_832851e1gy1foxonjqlsyj21o02t8x6q.jpg', 'mmchat/images/mw2048_832851e1gy1foxoswo4w9j229d29db2h.jpg', 'mmchat/images/mw2048_832851e1gy1foxongl7khj212p0t0wog.jpg', 'mmchat/images/mw2048_832851e1gy1foxou97bjvj229d29d7wk.jpg', 'mmchat/images/mw2048_832851e1gy1foxonujr34j22c02c1kjn.jpg', 'mmchat/images/mw2048_832851e1gy1foxonbhlqoj22z228a7wi.jpg'], 'metadata': {'dataset': 'twitter_post', 'split': 'train', 'num_sample': 0, 'task_instruction': '', 'question_type': 'open-ended'}}\n"
     ]
    }
   ],
   "source": [
    "print(not_conversation_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c0d51072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_gpt_and_human_conversation(item):\n",
    "    \"\"\"\n",
    "    Swap the 'from' field of the first and last conversation messages.\n",
    "\n",
    "    Parameters:\n",
    "        item (dict): The item containing conversations.\n",
    "\n",
    "    Returns:\n",
    "        dict: The item with swapped conversation roles.\n",
    "    \"\"\"\n",
    "    conversation = item[\"conversations\"]\n",
    "    for i,message in enumerate(conversation):\n",
    "        if message[\"from\"] == \"human\" and i % 2 == 1:\n",
    "            conversation[i][\"from\"] = \"gpt\"\n",
    "        elif message[\"from\"] == \"gpt\" and i % 2 == 0:\n",
    "            conversation[i][\"from\"] = \"human\"\n",
    "        else:\n",
    "            print(item)\n",
    "            raise ValueError(f\"Unknown conversation role: {message['from']}\")\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0583d268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5734 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5734/5734 [00:00<00:00, 27124.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "swap_data_list = []\n",
    "for d in tqdm(not_conversation_list):\n",
    "    swapped_item = swap_gpt_and_human_conversation(deepcopy(d))\n",
    "    swap_data_list.append(swapped_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e0fb92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'twitter_post', 'id': 0, 'conversations': [{'from': 'human', 'value': 'Help me write a Twitter post considering the following images.\\n<image><image><image><image><image><image><image><image><image>'}, {'from': 'gpt', 'value': '\"Embracing the serenity of island life where the water is as clear as the skies. üåä‚òÄÔ∏è #IslandVibes #CrystalClear #BeachDays #TravelDiaries\"'}], 'image': ['mmchat/images/mw2048_832851e1gy1foxoneylnaj22c02c07wi.jpg', 'mmchat/images/mw2048_832851e1gy1foxop6mkvgj22av2av7wi.jpg', 'mmchat/images/mw2048_832851e1gy1foxoo3z9v0j2276276hdu.jpg', 'mmchat/images/mw2048_832851e1gy1foxonjqlsyj21o02t8x6q.jpg', 'mmchat/images/mw2048_832851e1gy1foxoswo4w9j229d29db2h.jpg', 'mmchat/images/mw2048_832851e1gy1foxongl7khj212p0t0wog.jpg', 'mmchat/images/mw2048_832851e1gy1foxou97bjvj229d29d7wk.jpg', 'mmchat/images/mw2048_832851e1gy1foxonujr34j22c02c1kjn.jpg', 'mmchat/images/mw2048_832851e1gy1foxonbhlqoj22z228a7wi.jpg'], 'metadata': {'dataset': 'twitter_post', 'split': 'train', 'num_sample': 0, 'task_instruction': '', 'question_type': 'open-ended'}}\n",
      "{'datasource': 'twitter_post', 'id': 0, 'conversations': [{'from': 'gpt', 'value': 'Help me write a Twitter post considering the following images.\\n<image><image><image><image><image><image><image><image><image>'}, {'from': 'human', 'value': '\"Embracing the serenity of island life where the water is as clear as the skies. üåä‚òÄÔ∏è #IslandVibes #CrystalClear #BeachDays #TravelDiaries\"'}], 'image': ['mmchat/images/mw2048_832851e1gy1foxoneylnaj22c02c07wi.jpg', 'mmchat/images/mw2048_832851e1gy1foxop6mkvgj22av2av7wi.jpg', 'mmchat/images/mw2048_832851e1gy1foxoo3z9v0j2276276hdu.jpg', 'mmchat/images/mw2048_832851e1gy1foxonjqlsyj21o02t8x6q.jpg', 'mmchat/images/mw2048_832851e1gy1foxoswo4w9j229d29db2h.jpg', 'mmchat/images/mw2048_832851e1gy1foxongl7khj212p0t0wog.jpg', 'mmchat/images/mw2048_832851e1gy1foxou97bjvj229d29d7wk.jpg', 'mmchat/images/mw2048_832851e1gy1foxonujr34j22c02c1kjn.jpg', 'mmchat/images/mw2048_832851e1gy1foxonbhlqoj22z228a7wi.jpg'], 'metadata': {'dataset': 'twitter_post', 'split': 'train', 'num_sample': 0, 'task_instruction': '', 'question_type': 'open-ended'}}\n"
     ]
    }
   ],
   "source": [
    "print(swap_data_list[0])\n",
    "print(not_conversation_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5129789",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pass_data_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m path = \u001b[33m\"\u001b[39m\u001b[33m/data_ssd/M4-Instruct-Data/m4_instruct_annotations_fixed.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m saved_data = \u001b[43mpass_data_list\u001b[49m.extend(swap_data_list)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal saved data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pass_data_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'pass_data_list' is not defined"
     ]
    }
   ],
   "source": [
    "path = \"/data_ssd/M4-Instruct-Data/m4_instruct_annotations_fixed.json\"\n",
    "\n",
    "saved_data = pass_data_list.extend(swap_data_list)\n",
    "print(f\"Total saved data: {len(pass_data_list)}\")\n",
    "\n",
    "import os\n",
    "if not os.path.exists(os.path.dirname(path)):\n",
    "    os.makedirs(os.path.dirname(path))\n",
    "with open(path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(pass_data_list, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f86161d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed data length: 621548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 621548/621548 [07:41<00:00, 1345.43it/s] \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 621548/621548 [00:00<00:00, 673190.70it/s]\n"
     ]
    }
   ],
   "source": [
    "fixed_data = load_json(path)\n",
    "print(f\"Fixed data length: {len(fixed_data)}\")\n",
    "\n",
    "image_root_dir = \"/data_ssd/llava-onevision-data-symbolic-link\"\n",
    "\n",
    "break_flag = False\n",
    "for d in tqdm(fixed_data):\n",
    "    if type(d[\"image\"]) == str:\n",
    "        image_path_list = [d[\"image\"]]\n",
    "    elif type(d[\"image\"]) == list:\n",
    "        image_path_list = d[\"image\"]\n",
    "        \n",
    "    for image_path in image_path_list:\n",
    "        image_path = os.path.join(image_root_dir, image_path)\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image path does not exist: {image_path}\")\n",
    "            break_flag = True\n",
    "            break\n",
    "    if break_flag:\n",
    "        break\n",
    " \n",
    "from tqdm import tqdm\n",
    "pass_data_list = []\n",
    "not_pass_data_list = []\n",
    "for i, d in enumerate(tqdm(fixed_data)):\n",
    "    if not check_conversation_format(d) or not check_multiple_images(d):\n",
    "        not_pass_data_list.append(d)\n",
    "        continue\n",
    "    \n",
    "    pass_data_list.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfb21163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(not_pass_data_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5f71b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

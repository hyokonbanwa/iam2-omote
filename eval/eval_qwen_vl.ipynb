{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff9969dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omote/cluster_project/iam2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import imgviz\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Callable\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "import random\n",
    "import regex as re\n",
    "import torch\n",
    "\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torchvision.ops import box_iou\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "170d28d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device(is_ddp):\n",
    "    if is_ddp:\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "        device = torch.device(\"cuda:{}\".format(local_rank))\n",
    "        return device\n",
    "    else:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        return device\n",
    "def get_rank_size(is_ddp: bool):\n",
    "    if is_ddp:\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "        world_rank = int(os.environ[\"RANK\"])  # dist.get_rank()\n",
    "        world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "        return local_rank, world_rank, world_size\n",
    "    else:\n",
    "        return 0, 0, 1\n",
    "\n",
    "def get_dataloader(\n",
    "    is_ddp: bool,\n",
    "    dataset: Dataset,\n",
    "    shuffle: bool,\n",
    "    loader_drop_last: bool,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    collate_fn: Callable,\n",
    "    pin_memory: bool = None,\n",
    "    world_size: int = None,\n",
    "    world_rank: int = None,\n",
    "    seed: int = None,\n",
    "    sampler_drop_last: bool | None = None,\n",
    "):\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "\n",
    "    if is_ddp:\n",
    "        sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "            dataset, num_replicas=world_size, rank=world_rank, shuffle=shuffle, drop_last=sampler_drop_last, seed=seed\n",
    "        )\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=loader_drop_last,\n",
    "            sampler=sampler,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=g,\n",
    "        )\n",
    "    else:\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,\n",
    "            shuffle=shuffle,\n",
    "            drop_last=loader_drop_last,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=g,\n",
    "        )\n",
    "        \n",
    "class RefCOCODataset(Dataset):\n",
    "    def __init__(self, dataset, image_folder_root):\n",
    "        self.dataset = dataset\n",
    "        self.image_folder_root = image_folder_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        raw_image_info = json.loads(sample[\"raw_image_info\"])\n",
    "        image_file_name = raw_image_info[\"file_name\"]\n",
    "        original_image_width_height = (raw_image_info[\"width\"], raw_image_info[\"height\"])\n",
    "        image_path = os.path.join(self.image_folder_root,image_file_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        bbox = sample[\"bbox\"]\n",
    "        caption_list = [sentences[\"raw\"] for sentences in sample[\"sentences\"]]\n",
    "        \n",
    "        return image, caption_list, bbox,original_image_width_height,image_file_name\n",
    "    \n",
    "def eval_collate_fn(batch):\n",
    "    caption_list = []\n",
    "    image_list = []\n",
    "    bbox_list = []\n",
    "    original_image_width_height_list = []\n",
    "    image_file_name_list = []\n",
    "    for image, caption, bbox, original_image_width_height, image_file_name in batch:\n",
    "        image_list.append(image)\n",
    "        caption_list.append(caption)\n",
    "        bbox_list.append(bbox)\n",
    "        original_image_width_height_list.append(original_image_width_height)\n",
    "        image_file_name_list.append(image_file_name)\n",
    "        \n",
    "    return image_list, caption_list, bbox_list, original_image_width_height_list,image_file_name_list\n",
    "\n",
    "def fix_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    seedをする\n",
    "\n",
    "    Args:\n",
    "        seed (int): seed値\n",
    "    \"\"\"\n",
    "    # Python random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)  # cpuとcudaも同時に固定\n",
    "    torch.cuda.manual_seed(seed)  # 上記で呼び出される\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms = True\n",
    "    \n",
    "    \n",
    "def qwen_vl_conversation(caption,image):\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "\n",
    "                # {\"type\": \"text\", \"text\": \"Please output bbox coordinates and names of every object in this image in JSON format\"},\n",
    "                # {\"type\": \"text\", \"text\": f\"Locate \\\"{caption}\\\", report the bbox coordinates in JSON format.\"},\n",
    "                {\"type\": \"text\", \"text\": f\"Locate the region that corresponds to the following sentence.\\\"{caption}\\\" Report the bbox coordinates in JSON format.\"},\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": image,\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    return conversation\n",
    "\n",
    "def make_inputs(caption_list,image_list,processor):\n",
    "    conversation_list = []\n",
    "    prompt_list = []\n",
    "    for caption,image in zip(caption_list,image_list):\n",
    "        conversation = qwen_vl_conversation(caption,image)\n",
    "        conversation_list.append(conversation)\n",
    "        prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "        prompt_list.append(prompt)\n",
    "        \n",
    "    image_inputs, video_inputs = process_vision_info(conversation_list)\n",
    "    inputs = processor(\n",
    "        text=prompt_list,\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    model_image_width_height_list = []\n",
    "    for image in image_inputs:\n",
    "        model_image_width_height = (image.size[0], image.size[1])\n",
    "        model_image_width_height_list.append(model_image_width_height)\n",
    "    return inputs, model_image_width_height_list\n",
    "\n",
    "\n",
    "def extract_bbox_from_text(ans):\n",
    "    pattern = re.compile(r'\\[((\\d+,\\s*){3}(\\d+\\s*))\\]')\n",
    "    match_list = pattern.findall(ans)\n",
    "    if len(match_list) > 0:\n",
    "        answer = [list(map(float,match[0].split(\",\"))) for match in match_list]\n",
    "    else:\n",
    "        answer = \"FAILED\"\n",
    "    return answer\n",
    "\n",
    "def bbox_relative_to_absolute(relative_bbox, image_width_height):\n",
    "    width, height = image_width_height\n",
    "    x1 = relative_bbox[0] * width\n",
    "    y1 = relative_bbox[1] * height\n",
    "    x2 = relative_bbox[2] * width\n",
    "    y2 = relative_bbox[3] * height\n",
    "    absolute_bbox = [x1, y1, x2, y2]\n",
    "    return absolute_bbox\n",
    "\n",
    "def bbox_absolute_to_relative(absolute_bbox, image_width_height):\n",
    "    width, height = image_width_height\n",
    "    x1 = absolute_bbox[0] / width\n",
    "    y1 = absolute_bbox[1] / height\n",
    "    x2 = absolute_bbox[2] / width\n",
    "    y2 = absolute_bbox[3] / height\n",
    "    relative_bbox = [x1, y1, x2, y2]\n",
    "    return relative_bbox\n",
    "\n",
    "def get_bbox_from_output(output_text, model_image_width_height):\n",
    "    absolute_bbox = extract_bbox_from_text(output_text)\n",
    "    if absolute_bbox == \"FAILED\":\n",
    "        return False\n",
    "    else:\n",
    "        absolute_bbox = absolute_bbox[0]\n",
    "        relative_bbox = bbox_absolute_to_relative(absolute_bbox, model_image_width_height)\n",
    "        return relative_bbox\n",
    "\n",
    "def calculate_iou(bbox1, bbox2):\n",
    "    \"\"\"\n",
    "    Calculate IoU for two lists of bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        bbox_list1 (list): List of bounding boxes in the format [x1, y1, x2, y2].\n",
    "        bbox_list2 (list): List of bounding boxes in the format [x1, y1, x2, y2].\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: IoU matrix where each element (i, j) represents the IoU between bbox_list1[i] and bbox_list2[j].\n",
    "    \"\"\"\n",
    "    iou_matrix = box_iou(torch.tensor([bbox1]).float(), torch.tensor([bbox2]).float())\n",
    "    iou_list = iou_matrix.diagonal().tolist()\n",
    "    return iou_list[0]\n",
    "\n",
    "def create_jsonl_file(file_path):\n",
    "    \"\"\"\n",
    "    Create an empty JSONL file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the JSONL file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        pass\n",
    "\n",
    "def append_to_jsonl(file_path, data):\n",
    "    \"\"\"\n",
    "    Append a dictionary as a JSON object to a JSONL file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the JSONL file.\n",
    "        data (dict): Dictionary to append as a JSON object.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'a') as f:\n",
    "        json.dump(data, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "def save_json(file_path, data):\n",
    "    \"\"\"\n",
    "    Save data to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file.\n",
    "        data (dict): Data to save.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "        \n",
    "    \n",
    "def calculate_accuracy(iou, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate accuracy based on IoU threshold.\n",
    "\n",
    "    Args:\n",
    "        iou (float): IoU value.\n",
    "        threshold (float): IoU threshold for accuracy.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if IoU is greater than or equal to the threshold, False otherwise.\n",
    "    \"\"\"\n",
    "    return iou >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c73ff4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 34.26it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_key = \"validation\"  # \"test\" or \"testB\"\n",
    "fix_seed(0)\n",
    "device = get_device(is_ddp=False)\n",
    "\n",
    "model_root_dir = \"/home/omote/local-share-data_ssd/huggingface_model_weights\"\n",
    "model_id = os.path.join(model_root_dir, \"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "cache_dir = \"/home/omote/local-share-data_ssd/huggingface_cache\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_id, cache_dir=cache_dir, torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "dataset_root_dir = \"/home/omote/local-share-data_ssd/huggingface_dataset\"\n",
    "image_folder_root = \"/home/omote/local-share-data/mscoco2014/train2014\"\n",
    "\n",
    "\n",
    "dataset_id = os.path.join(dataset_root_dir,\"jxu124/refcoco\")\n",
    "dataset = load_dataset(dataset_id, cache_dir=cache_dir)\n",
    "\n",
    "\n",
    "eval_dataset_key = [\"validation\",\"test\",\"testB\"]\n",
    "eval_dataset_dict = {k: RefCOCODataset(dataset[k],image_folder_root) for k in eval_dataset_key}\n",
    "\n",
    "eval_dataset = eval_dataset_dict[dataset_key]\n",
    "eval_dataloader = DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=16,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        collate_fn=eval_collate_fn,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73081ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/239 [00:00<?, ?it/s]/home/omote/cluster_project/iam2/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  0%|          | 0/239 [00:05<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IoU: 0.8383614122867584\n",
      "Mean Accuracy: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_output_jsonl_path = os.path.join(\"./\",f\"qwen_vl_refcoco_{dataset_key}.jsonl\")\n",
    "score_json_path = os.path.join(\"./\",f\"qwen_vl_refcoco_score_{dataset_key}.json\")\n",
    "create_jsonl_file(model_output_jsonl_path)\n",
    "\n",
    "iou_list = []\n",
    "accuracy_list = []\n",
    "for image_list, caption_list, bbox_list, original_image_width_height_list,image_file_name_list in tqdm.tqdm(eval_dataloader):\n",
    "    caption_list = [caption[0] for caption in caption_list]\n",
    "    inputs,model_image_width_height_list = make_inputs(caption_list,image_list,processor)\n",
    "    inputs = inputs.to(model.device)\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=400, do_sample=False,temperature=0)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    generated_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=False,)\n",
    "    \n",
    "    model_bbox_list = []\n",
    "    gt_bbox_list = []\n",
    "    for i in range(len(image_file_name_list)):\n",
    "        model_bbox = get_bbox_from_output(generated_text[i],model_image_width_height_list[i])\n",
    "        model_bbox_list.append(model_bbox)\n",
    "        gt_bbox = bbox_absolute_to_relative(bbox_list[i], original_image_width_height_list[i])\n",
    "        gt_bbox_list.append(gt_bbox)\n",
    "        if model_bbox == False:\n",
    "            iou = 0.0\n",
    "        else:        \n",
    "            iou = calculate_iou(model_bbox, gt_bbox)\n",
    "            \n",
    "        accuracy = calculate_accuracy(iou, threshold=0.5)\n",
    "        image_file_name = image_file_name_list[i]\n",
    "        caption = caption_list[i]\n",
    "        \n",
    "        data = {\n",
    "            \"image_file_name\": image_file_name,\n",
    "            \"caption\": caption,\n",
    "            \"bbox\": gt_bbox,\n",
    "            \"model_bbox\": model_bbox,\n",
    "            \"iou\": iou,\n",
    "            \"accuracy\": accuracy\n",
    "        }\n",
    "        \n",
    "        append_to_jsonl(model_output_jsonl_path, data)\n",
    "        iou_list.append(iou)\n",
    "        accuracy_list.append(accuracy)\n",
    "    break\n",
    "mean_iou = np.mean(iou_list)\n",
    "print(f\"Mean IoU: {mean_iou}\")\n",
    "\n",
    "mean_accuracy = np.mean(accuracy_list)\n",
    "print(f\"Mean Accuracy: {mean_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79e6e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgviz\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def visualize_bbox(image, bbox, caption,original_image_width_height):\n",
    "    if original_image_width_height:\n",
    "        width, height = original_image_width_height\n",
    "        bbox = [bbox[0] * width, bbox[1] * height, bbox[2] * width, bbox[3] * height]\n",
    "        bboxes = np.array([bbox[1],bbox[0],bbox[3],bbox[2]]).astype(np.int32).reshape(-1, 4)\n",
    "    else:\n",
    "        bboxes = np.array([bbox[1],bbox[0],bbox[3],bbox[2]]).astype(np.int32).reshape(-1, 4)\n",
    "    labels = [2]\n",
    "    image = imgviz.instances2rgb(np.array(image), bboxes=bboxes, labels=labels,captions=[caption],font_size=16)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f3bd11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(image_list)):\n",
    "#     if model_bbox_list[i] == False:\n",
    "#         print(\"model_bbox_list[i] == False\")\n",
    "#     else:\n",
    "#         visualize_bbox(image_list[i], model_bbox_list[i], caption_list[i],original_image_width_height_list[i])\n",
    "#     visualize_bbox(image_list[i], gt_bbox_list[i], caption_list[i],original_image_width_height_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53d8159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' addCriterion(\"bowl behind the others can only see part\", {\"bbox_2d\": [119, 0, 463, 145], \"label\": \"bowl behind the others can only see part\"})<|im_end|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', ' addCriterion \"front bowl w/carrots in it\" 的区域，其边界框坐标如下：\\n\\n```json\\n[\\n\\t{\"bbox_2d\": [163, 137, 614, 415], \"label\": \"front bowl w/carrots in it\"}\\n]\\n```<|im_end|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', ' addCriterion \"little girl\" 的区域，其边界框坐标为：```json\\n[\\n\\t{\"bbox_2d\": [117, 304, 295, 476], \"label\": \"little girl\"}\\n]\\n```<|im_end|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', ' addCriterion \"green woman\" 的bbox为：```json\\n[\\n\\t{\"bbox_2d\": [301, 109, 578, 476], \"label\": \"green woman\"}\\n]\\n```<|im_end|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', ' addCriterion \"mom\" 的区域。返回的 JSON 格式为：```json\\n[\\n\\t{\"bbox_2d\": [146, 178, 375, 609], \"label\": \"mom\"}\\n]\\n```<|im_end|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', ' addCriterion \"child sitting on womans lap\" 的bbox为：```json\\n[\\n\\t{\"bbox_2d\": [157, 256, 280, 479], \"label\": \"child sitting on womans lap\"}\\n]\\n```<|im_end|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', ' addCriterion \"left guy\" 的位置，其在图像中的边界框坐标为：```json\\n[\\n\\t{\"bbox_2d\": [0, 31, 324, 356], \"label\": \"left guy\"}\\n]\\n```<|im_end|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', ' addCriterion \"lady on right\" 的区域，其边界框坐标为：```json\\n[\\n\\t{\"bbox_2d\": [210, 8, 345, 249], \"label\": \"lady on right\"}\\n]\\n```<|im_end|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', ' addCriterion \"bottom box left dude\" 的区域，其边界框坐标为：```json\\n[\\n\\t{\"bbox_2d\": [30, 291, 278, 498], \"label\": \"bottom box left dude\"}\\n]\\n```<|im_end|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', ' addCriterion \"top picture on the left\" to the image, the corresponding bounding box is:\\n```json\\n[\\n\\t{\"bbox_2d\": [36, 70, 214, 250], \"label\": \"top picture on the left\"}\\n]\\n```<|im_end|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', ' addCriterion \"bottom right guy\" 的bbox coordinates in JSON format: `{\"bbox_2d\": [103, 264, 364, 504], \"label\": \"bottom right guy\"}`<|im_end|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', ' addCriterion \"cat on top\" 的区域，其边界框坐标为：```json\\n[\\n\\t{\"bbox_2d\": [147, 69, 280, 203], \"label\": \"cat on top\"}\\n]\\n```<|im_end|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', '```json\\n[\\n\\t{\"bbox_2d\": [158, 483, 420, 644], \"label\": \"black cat under sink\"},\\n\\t{\"bbox_2d\": [157, 74, 279, 202], \"label\": \"black cat under sink\"}\\n]\\n```<|im_end|>', ' addCriterion \"bbox_2d\": {\"bbox_2d\": [310, 27, 436, 371], \"label\": \"Man standing\"}<|im_end|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', ' addCriterion \"person in chair at left\" 的区域，其边界框坐标如下：\\n\\n```json\\n[\\n\\t{\"bbox_2d\": [78, 95, 340, 364], \"label\": \"person in chair at left\"}\\n]\\n```<|im_end|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', ' addCriterion \"gurl\" 的位置，其边界框坐标为：```json\\n[\\n\\t{\"bbox_2d\": [153, 446, 337, 644], \"label\": \"gurl\"}\\n]\\n```<|im_end|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6521eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "\n",
    "            # {\"type\": \"text\", \"text\": \"Please output bbox coordinates and names of every object in this image in JSON format\"},\n",
    "            {\"type\": \"text\", \"text\": \"Locate \\\"The creature on the left side of the woman\\\", report the bbox coordinates in JSON format.\"},\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "295c7a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d11fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "print(image_inputs)\n",
    "print(video_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ede74ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda:0\")\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d17de1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9a9d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bbox_from_text(ans):\n",
    "    pattern = re.compile(r'\\[(((0|1)\\.(\\d){3}\\,){3}((0|1)\\.(\\d){3}))\\]')\n",
    "    match_list = pattern.findall(ans)\n",
    "\n",
    "    if len(match_list) > 0:\n",
    "        answer = [list(map(float,match[0].split(\",\"))) for match in match_list]\n",
    "    else:\n",
    "        answer = \"FAILED\"\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6039600",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = processor.tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7afcc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90bebbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab[\"<|box_start|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b297f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgviz\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def visualize_bbox(image, bbox, caption):\n",
    "    bboxes = np.array([bbox[1],bbox[0],bbox[3],bbox[2]]).astype(np.int32).reshape(-1, 4)\n",
    "    labels = [2]\n",
    "    image = imgviz.instances2rgb(np.array(image), bboxes=bboxes, labels=labels,captions=[caption],font_size=16)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "visualize_bbox(image_inputs[0], [458, 586, 1190, 1213], \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5583ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "original_image = Image.open(\"../demo.jpeg\")\n",
    "print(original_image.size)\n",
    "print(image_inputs[0].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56904abe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
